---
title: "Your Document Title"
output:
  md_document:
    variant: gfm
knit: (function(input, ...) { rmarkdown::render(input, ..., output_file = "ReadMe.md") })
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  fig.align = "center",
  fig.width = 8,   
  fig.height = 6, 
  dpi = 300,     
  dev = "png",
  out.width = "70%",
  out.height = "70%"
)

knitr::knit_hooks$set(plot = function(x, options) {
  paste0('<div align="center">\n',
         '<img src="', x, '" width="70%">\n',
         '</div>')
})
```

```{r}
library(tidyverse)
library(caret)
library(randomForest)
```

Given the following data.

```{r}
data <- read.csv(file="data.csv",header=TRUE)
```

```{r echo=FALSE}
library(knitr)
library(kableExtra)

your_tibble <- head(data,5)
kable(your_tibble, format = "html") %>%
  kable_styling(position = "center") %>% 
  save_kable(file = "~/Desktop/DS_DA_Projects/Marketing_Prediction/ReadMe_files/figure-gfm/A1.png", zoom = 2)
knitr::include_graphics("~/Desktop/DS_DA_Projects/Marketing_Prediction/ReadMe_files/figure-gfm/A1.png")
```

Let's build a Random Forest binary classifier.

Step 1: Clean the data

```{r}
data <- data %>%
  mutate(across(c(4,5,6), ~ifelse(is.na(.) | . == "", "unknown", .)))

data$income <- as.integer(as.character(factor(data[, "income"],
                      levels = c("unknown", "Under $10k", "10-19,999", "20-29,999", 
                                 "30-39,999", "40-49,999", "50-59,999", "60-69,999", "70-79,999",
                                 "80-89,999", "90-99,999", "100-149,999", "150 - 174,999",
                                 "175 - 199,999", "200 - 249,999", "250k+"), 
                      labels = c("55000",             # replacing unkown with the median income level 
                                 "5000",         # midpoint of "Under $10k"
                                 "15000",        # midpoint of "10-19,999"
                                 "25000",        # midpoint of "20-29,999"
                                 "35000",        # midpoint of "30-39,999"
                                 "45000",        # midpoint of "40-49,999"
                                 "55000",        # midpoint of "50-59,999"
                                 "65000",        # midpoint of "60-69,999"
                                 "75000",        # midpoint of "70-79,999"
                                 "85000",        # midpoint of "80-89,999"
                                 "95000",        # midpoint of "90-99,999"
                                 "125000",       # midpoint of "100-149,999"
                                 "162500",       # midpoint of "150 - 174,999"
                                 "187500",       # midpoint of "175 - 199,999"
                                 "225000",       # midpoint of "200 - 249,999"
                                 "250000"))))   # custom value for "250k+"


data$gender <- factor(data$gender, levels = c("M","F","unknown"), labels = c("1","0","0")) #replacing unkown with female

data$marital_status <- factor(data$marital_status, levels = c("M","S","unknown"), labels = c("1","0","1")) # replacing unkown with married

data$target <- factor(data$target, levels = c(0,1), labels = c(0,1))

data$dist <- as.integer(data$dist) # make this an integer
```

```{r}
data
```

Step 2: Build a model

```{r}
set.seed(123)
trainIndex <- createDataPartition(data$target, p = 0.8, list = FALSE, times = 1)

train <- data[trainIndex,]
test <- data[-trainIndex,]

rf_model <- randomForest(target ~ ., data = train, importance = TRUE, ntree = 500)

predictions <- predict(rf_model, newdata = test)

conf_matrix <- confusionMatrix(predictions, test$target)
```

```{r}
conf_matrix
```

The best accuracy I could get by fitting the data to the model was 0.7662

```{r}
model_output <- predict(rf_model, newdata = data, type = "prob")
data$prob_one <- model_output[,2]
```

```{r}
lift_data <- data[order(data$prob_one,decreasing = TRUE),] #order from greatest probability of response to least

sum_reponses <- sum(as.numeric(as.character(data$target))) # there are 1000 totall responses in case you want to hardcode

lift_curve <- numeric(0)

baseline_curve <- numeric(0)

for (i in 0:10) {
  
      lift_curve[i+1] =  sum(lift_data[seq(0, (0.1*i) * nrow(data_lift)), 7] == "1") / sum_reponses*100
      
      baseline_curve[i+1] = sum(data[seq(0, (0.1*i) * nrow(data)), 7] == "1") / sum_reponses*100

}

lift_chart_data <- data.frame(lift = lift_curve, baseline = baseline_curve)

ggplot(lift_chart_data,aes(x=seq(0,1,.1), y=lift_curve)) + geom_line(color="blue") + 
                       geom_line(aes(x=seq(0,1,.1), y=baseline_curve),color="red") + theme_minimal() + 
  labs(x = "Percentage of Customers Contacted", y = "Percentage of Customers Respond")


advantage_curve_data <- lift_chart_data %>%
    mutate(advantage = coalesce(lift_curve / baseline, 0))


advantage_curve_data

ggplot(lift_chart_data,aes(x=seq(0,1,.1), y=lift_curve)) + geom_line(color="blue") + 
                       geom_line(aes(x=seq(0,1,.1), y=baseline_curve),color="red") + theme_minimal() + 
  labs(x = "Percentage of Customers Contacted", y = "Percentage of Customers Respond")

```
