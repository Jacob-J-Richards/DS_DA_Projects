---
title: "Your Document Title"
output:
  md_document:
    variant: gfm
knit: (function(input, ...) { rmarkdown::render(input, ..., output_file = "ReadMe.md") })
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  fig.align = "center",
  fig.width = 8,
  fig.height = 6,
  dpi = 300,
  dev = "png",
  out.width = "70%",
  out.height = "70%"
)

knitr::knit_hooks$set(plot = function(x, options) {
  paste0('<div align="center">\n',
         '<img src="', x, '" width="70%">\n',
         '</div>')
})
```

# Transaction Anomaly Detection

## Introduction

You work as a Data Scientist at a Payments processor company. On February 14th 2020, you get calls from many merchants reporting that they are seeing a spike in customer complaints and want you to check if there are any issues.

## Data set

The data available to you is the transaction count and how many of which were successful for each combination of categorical variables (Bank, Payment Gateway, Merchant, Payment Method, etc..) for each hour within the 72 hours from February 12th to 14th.

```{r, include=FALSE}
setwd("/Users/jacobrichards/Desktop/DS_DA_Projects/Anamoly_Detection")
transactions <- read.csv(file = "transactions.csv", na.strings = c("", "NA"))

library(knitr)
library(kableExtra)

your_tibble <- head(transactions, 5)

kable(your_tibble, format = "html") %>%
  kable_styling(position = "center") %>% 
  save_kable(
    file = "~/Desktop/DS_DA_Projects/Anamoly_Detection/ReadMe_files/figure-gfm/AD_1.png", 
    zoom = 2
  )

```

```{r echo=FALSE}
knitr::include_graphics(
  "~/Desktop/DS_DA_Projects/Anamoly_Detection/ReadMe_files/figure-gfm/AD_1.png"
)
```

## Initial Approach

We're not sure what we're looking for yet, so let's plot the percentage of failed transactions for each of the 72 hours within the entire data set.

clean the data

```{r}
library('ggplot2')

setwd("/Users/jacobrichards/Desktop/DS_DA_Projects/Anamoly_Detection")
transactions <- read.csv(file = "transactions.csv", na.strings = c("", "NA"))
transactions[is.na(transactions)] <- "notprovided"

data <- transactions
colnames(data) <- c("t", "s", "mid", "pmt", "pg", "subtype", "hr", "bank")

```

compute failure rate for each hour

```{r}
unique_hours <- unique(data$hr)

t <- aggregate(data$t, by = list(data$hr), sum)
s <- aggregate(data$s, by = list(data$hr), sum)

f <- t[, 2] - s[, 2]
failure_rate <- f / t[, 2]
failure_count <- f

unique_hours <- sort(unique_hours)

```

plot failure rate for each hour

```{r echo=FALSE}
failed_transactions_rate <- data.frame(
  hours = unique_hours, 
  failedTransactions = failure_rate, 
  x_index = seq(1, 72, by = 1)
)

ggplot(data = failed_transactions_rate, aes(x = x_index, y = failedTransactions)) + 
  geom_area(fill = "blue", alpha = 0.25) + 
  geom_line(color = "black") +  
  scale_x_continuous(
    breaks = seq(1, 72, by = 6), 
    minor_breaks = 1:72, 
    labels = unique_hours[seq(1, length(unique_hours), by = 6)]
  ) + 
  coord_cartesian(ylim = range(failed_transactions_rate$failedTransactions, na.rm = TRUE)) +  
  labs(
    title = "Failed Transactions Percentage by Hour", 
    x = "Hour (72)", 
    y = "Failed Transactions Per Hour"
  ) +
  theme(
    axis.text.x = element_text(angle = 60, hjust = 1, size = 8), 
    axis.title.x = element_text(size = 10), 
    axis.title.y = element_text(size = 10),
    plot.background = element_rect(fill = "white", color = NA),
    panel.background = element_rect(fill = "white", color = NA),
    panel.grid.major.x = element_blank(), 
    panel.grid.minor.x = element_blank(), 
    panel.grid.major.y = element_blank(), 
    legend.position = "none"
  )
```

So we do see a spike in transaction failure rates up to 45% afternoon/overnight the day before we started receiving complaints.

Clearly there is a problem here, but we need to find precisely what caused this so the problem can be addressed.

plotting total transaction failures for each hour

```{r echo=FALSE}
failed_transactions <- data.frame(
  hours = unique_hours, 
  failedTransactions = failure_count, 
  x_index = seq(1, 72, by = 1)
)

ggplot(data = failed_transactions, aes(x = x_index, y = failedTransactions)) + 
  geom_area(fill = "blue", alpha = 0.25) + 
  geom_line(color = "black") +  
  scale_x_continuous(
    breaks = seq(1, 72, by = 6), 
    minor_breaks = 1:72, 
    labels = unique_hours[seq(1, length(unique_hours), by = 6)]
  ) + 
  coord_cartesian(ylim = range(failed_transactions$failedTransactions, na.rm = TRUE)) +  
  labs(
    title = "Failed Transactions Counts by Hour", 
    x = "Hour (72)", 
    y = "Failed Transactions Per Hour"
  ) +
  theme(
    axis.text.x = element_text(angle = 60, hjust = 1, size = 8), 
    axis.title.x = element_text(size = 10), 
    axis.title.y = element_text(size = 10),
    plot.background = element_rect(fill = "white", color = NA),
    panel.background = element_rect(fill = "white", color = NA),
    panel.grid.major.x = element_blank(), 
    panel.grid.minor.x = element_blank(), 
    panel.grid.major.y = element_blank(), 
    legend.position = "none"
  )

```

This isn't noteworthy, the failure counts are higher during the day because that's when customers are active.

To narrow down what caused our failure rate spike, we employ the Mahalanobis distance method which is useful for the detection of anomalies. Just as you can detect outliers in a variable of one dimension by an unusually high or low z-score, you can detect outliers in higher dimensional variables by it's z-score within it's higher dimensional distribution.

The variables we will produce a distribution of will be failure percentage and transaction count, such that observations of the data set where both of these values are unusually high will be identified as anomalies.

```{r echo=FALSE}
before_anamoly_detection_data <- data

data$failures <- data[, 1] - data[, 2]
data$failure_rate <- (data[, 1] - data[, 2]) / data[, 1]

kable(head(data, 3), format = "html") %>%
  kable_styling(position = "center") %>% 
  save_kable(
    file = "~/Desktop/DS_DA_Projects/Anamoly_Detection/ReadMe_files/figure-gfm/appended_1.png", 
    zoom = 2
  )

knitr::include_graphics(
  "~/Desktop/DS_DA_Projects/Anamoly_Detection/ReadMe_files/figure-gfm/appended_1.png"
)
```

Now that we have all of our variables prepared, we can form a two-dimensional distribution from them to find which observations are the greatest outliers.

Here is a plot of that distribution, as you can see the vast majority of the data is concentrated in the back corner of the volume in that very thin sliver.

The thin purple layer covering the rest of the x-y plane are where those outlier are that we're looking for.

To clarify, this is what a the 2-dimensional distribution of 10,000 samples for a standard Gaussian distribution looks like for comparison.

```{r eval=FALSE, include=FALSE}

distribution <- data.frame(failures = data$failures,rate = data$failure_rate )

library(plotly)
library(MASS)


kde <- kde2d(data$failures, data$failure_rate, n = 50)

  plot_ly(
    x = kde$x,
    y = kde$y,
    z = kde$z,
    type = "surface"
  )
  
kde <- kde2d(rnorm(10000), rnorm(10000), n = 50)

  plot_ly(
    x = kde$x,
    y = kde$y,
    z = kde$z,
    type = "surface"
  )
  
```

```{r include=FALSE}
knitr::include_graphics(
  "/Users/jacobrichards/Desktop/DS_DA_Projects/Anamoly_Detection/ReadMe_files/figure-gfm/3Ddistribution.png"
)

knitr::include_graphics(
  "/Users/jacobrichards/Desktop/DS_DA_Projects/Anamoly_Detection/ReadMe_files/figure-gfm/Gaussian_3d.png"
)
```

Evaluating the Mahalanobis method to find those outliers

```{r}
features <- data[, c("t", "failure_rate")]

center <- colMeans(features)
cov_matrix <- cov(features)

mahalanobis_distances <- mahalanobis(features, center, cov_matrix)

data$mahalanobis_score <- mahalanobis_distances

data <- data[order(data$mahalanobis_score, decreasing = TRUE), ]
top_quartile <- quantile(data$mahalanobis_score, 0.999)
filtered_data <- data[data$mahalanobis_score >= top_quartile, ]

```

Table of the 10 observations found to have the greatest outlier score.

```{r echo=FALSE}
your_tibble <- head(filtered_data, 10)

kable(your_tibble, format = "html") %>%
  kable_styling(position = "center") %>% 
  save_kable(
    file = "~/Desktop/DS_DA_Projects/Anamoly_Detection/ReadMe_files/figure-gfm/mscoreog.png", 
    zoom = 2
  )

knitr::include_graphics(
  "~/Desktop/DS_DA_Projects/Anamoly_Detection/ReadMe_files/figure-gfm/mscoreog.png"
)
```

Notice how the top 10 outliers all have a PAYTM service as the payment gateway with only difference in the variable name being the addition of the suffix \_UPI to PAYTM.

From that I deduced that the anomaly would be present in the PAYTM payment gateways being PAYTM, PAYTM_V2, and PAYTM_UPI.

The failure rate of PAYTM services for the payment gateway, and the other most common variables present in the top 10 anomalous observations is as follows.

```{r}
data <- before_anamoly_detection_data

first_subset <- data[
  (data[, 4] == c("UPI")) & 
  (data[, 5] %in% c("PAYTM", "PAYTM_V2", "PAYTM_UPI")) & 
  (data[, 6] == "UPI_PAY") & 
  (data[, 8] == "UPI"), 
]

```

```{r echo=FALSE}

unique_hours <- unique(data$hr)
unique_hours <- sort(unique_hours)

t <- aggregate(first_subset$t, by = list(first_subset$hr), sum)
s <- aggregate(first_subset$s, by = list(first_subset$hr), sum)
f <- t[, 2] - s[, 2]

proportion <- f / t[, 2] * 100

failed_transactions <- data.frame(
  hours = unique_hours, 
  failedTransactions = proportion, 
  x_index = seq(1, 72, by = 1))


ggplot(data = failed_transactions, aes(x = x_index, y = failedTransactions)) + 
  geom_area(fill = "blue", alpha = 0.25) + 
  geom_line(color = "black") + 
  scale_x_continuous(
    breaks = seq(1, 72, by = 6), 
    minor_breaks = 1:72, 
    labels = unique_hours[seq(1, length(unique_hours), by = 6)]
  ) + 
  coord_cartesian(ylim = range(failed_transactions$failedTransactions, na.rm = TRUE)) +  
  labs(
    title = "Failed Transactions Percentage by Hour", 
    x = "Hour (72)", 
    y = "Failed Transactions Per Hour"
  ) +
  theme(
    axis.text.x = element_text(angle = 60, hjust = 1, size = 8), 
    axis.title.x = element_text(size = 10),
    axis.title.y = element_text(size = 10), 
    plot.background = element_rect(fill = "white", color = NA),
    panel.background = element_rect(fill = "white", color = NA), 
    panel.grid.major.x = element_blank(),  
    panel.grid.minor.x = element_blank(), 
    panel.grid.major.y = element_blank(), 
    legend.position = "none")
```

Unfortunately, the result is white noise. This is not the exact problematic combination. Rather than checking each possible combination of the variables we believe the anomaly to be contained within manually, the following quick and dirty code will produce a failure rate plot for all combinations of payment methods and sub-types with each of the PAYTM payment gateways.

```{r}
payment_methods <- unique(data[, 4]) # All payment methods
subtypes <- unique(data[, 6])        # All subtypes
filter_values <- c("PAYTM", "PAYTM_V2", "PAYTM_UPI") # all combinations of the previous variables with each of these

# Generate combinations for each filtered value
combinations <- expand.grid(
  Payment_Method = character(),
  Subtype = character(),
  PMT_Values = character(),
  stringsAsFactors = FALSE
)

subset_list <- list()

# Create subsets for each combination
for (pmt in payment_methods) {
  for (st in subtypes) {
    for (fv in filter_values) {
      subset_name <- paste(pmt, fv, st, sep = " | ")
      subset_data <- data[
        data[, 4] == pmt & 
        data[, 6] == st & 
        data[, 5] == fv, 
      ]
      subset_list[[subset_name]] <- subset_data
    }
  }
}
cat("number of combinations produced:",(length(subset_list)))
```

plotting the combinations for which there are corresponding transactions within the data set for.

```{r}
par(mfrow = c(3,3))

for (subset_name in names(subset_list)) {
  subset_data <- subset_list[[subset_name]]
  
  if (nrow(subset_data) > 0) { 
    t <- aggregate(subset_data$t, by = list(subset_data$hr), sum)
    s <- aggregate(subset_data$s, by = list(subset_data$hr), sum)
    f <- t[, 2] - s[, 2] 
    
    proportion <- f / t[, 2] * 100
    
    plot(
      x = seq(1, nrow(t), by = 1), 
      y = proportion, 
      main = paste("", subset_name),
      type = "l"
    )
    
    unique_pmt <- unique(subset_data[, 5])
    combinations <- rbind(
      combinations, 
      data.frame(
        Payment_Method = unique(subset_data[, 4]),
        Subtype = unique(subset_data[, 6]),
        PMT_Values = paste(unique_pmt, collapse = ", "),
        stringsAsFactors = FALSE
      )
    )
  }
}

par(mfrow = c(1, 1)) 
```

of which only 8 of these combinations have any transactions to plot, clearly what we're looking for is the subset UPI \| PAYTM_V2 \| UPI_COLLECT for payment method, payment gateway, and sub-type respectively.

Better plot of the problematic subset of the data.

```{r}
paytm_subset <- data[
  (data[, 5] %in% c("PAYTM_V2")) & 
  (data[, 6] %in% c("UPI_COLLECT")) & 
  (data[, 4] == "UPI"), 
]

unique_hours <- unique(data$hr)
unique_hours <- sort(unique_hours)

t <- aggregate(paytm_subset$t, by = list(paytm_subset$hr), sum)
s <- aggregate(paytm_subset$s, by = list(paytm_subset$hr), sum)
f <- t[, 2] - s[, 2]

proportion <- f / t[, 2] * 100

failed_transactions <- data.frame(
  hours = unique_hours, 
  failedTransactions = proportion, 
  x_index = seq(1, 72, by = 1)
)

ggplot(data = failed_transactions, aes(x = x_index, y = failedTransactions)) + 
  geom_area(fill = "blue", alpha = 0.25) + 
  geom_line(color = "black") + 
  scale_x_continuous(
    breaks = seq(1, 72, by = 6), 
    minor_breaks = 1:72, 
    labels = unique_hours[seq(1, length(unique_hours), by = 6)]
  ) + 
  coord_cartesian(ylim = range(failed_transactions$failedTransactions, na.rm = TRUE)) +  
  labs(
    title = "Failed Transactions Percentage by Hour", 
    x = "Hour (72)", 
    y = "Failed Transactions Per Hour"
  ) +
  theme(
    axis.text.x = element_text(angle = 60, hjust = 1, size = 8), 
    axis.title.x = element_text(size = 10),
    axis.title.y = element_text(size = 10), 
    plot.background = element_rect(fill = "white", color = NA),
    panel.background = element_rect(fill = "white", color = NA), 
    panel.grid.major.x = element_blank(),  
    panel.grid.minor.x = element_blank(), 
    panel.grid.major.y = element_blank(), 
    legend.position = "none"
  )

```

The failure rate spike occurred from 5pm on the 13th to 6am on the 14th the same day that merchants reported customer complaints, during which, failure rates were at a minimum of 66% and a maximum of 85% for an entire 13 hours.

Which merchants were impacted by this anomaly?

```{r echo=FALSE}
paytm_subset <- data[(data[, 5] %in% c("PAYTM", "PAYTM_V2", "PAYTM_UPI", "notprovided")) & 
                     (data[, 6] %in% c("UPI_COLLECT")) & 
                     (data[, 4] == "UPI"), ]

paytm_subset$failure_sum <- paytm_subset[, 1] - paytm_subset[, 2]

failure_sum_by_merchant <- aggregate(paytm_subset[, 9], by = list(paytm_subset$mid), sum)
transaction_sum_by_merchant <- aggregate(paytm_subset[, 1], by = list(paytm_subset$mid), sum)

failure_sum_by_merchant$transction_sum <- transaction_sum_by_merchant[, 2]
failure_sum_by_merchant$failue_rate_merchant <- failure_sum_by_merchant[, 2] / failure_sum_by_merchant[, 3]
subset_failures_by_merchant <- failure_sum_by_merchant

data$failures <- data[, 1] - data[, 2]
rest_of_data_set_failure_count_by_merchant <- aggregate(data[, 9], by = list(data$mid), sum)
rest_of_data_transaction_count_by_merchant <- aggregate(data[, 1], by = list(data$mid), sum)

rest_of_data_transaction_count_by_merchant$failure_rate <- rest_of_data_set_failure_count_by_merchant[, 2] / rest_of_data_transaction_count_by_merchant[, 2]
entire_set_failures_by_merchant <- rest_of_data_transaction_count_by_merchant
entire_set_failures_by_merchant$subset_rate <- subset_failures_by_merchant[, 4]
entire_set_failures_by_merchant$diff <- entire_set_failures_by_merchant[, 4] - entire_set_failures_by_merchant[, 3]

colnames(entire_set_failures_by_merchant) <- c(
  "Merchant", 
  "Failures", 
  "Failure_rate_Pre", 
  "Failure_Rate_Anamoly", 
  "Failure_Rate_Difference"
)

entire_set_failures_by_merchant <- entire_set_failures_by_merchant[c(
  "Merchant", 
  "Failure_rate_Pre", 
  "Failure_Rate_Anamoly", 
  "Failure_Rate_Difference"
)]

library('reshape2')
long_set_failures_by_merchant <- melt(
  data = entire_set_failures_by_merchant, 
  id.vars = c("Merchant"),
  measured.vars = c(
    "Failure_rate_Before_Anamoly", 
    "Failure_Rate_During_Anamoly", 
    "Difference_between_Failure_Rate"
  ),
  variable.name = "Before_after_difference", 
  value.name = "Rate"
)

ggplot(data = long_set_failures_by_merchant, 
  aes(x = Before_after_difference, y = Rate, fill = Merchant)) +
  geom_bar(stat = "identity", position = "dodge") +
  scale_fill_brewer(palette = "Set2") +  
  labs(
    title = "Failure Rates Before and During Anomaly by Merchant",
    x = "Period", 
    y = "Failure Rate"
  ) +
  theme_minimal()
```

In tabular form

```{r echo=FALSE}
library(webshot2)
library(gt)

gt_table <- entire_set_failures_by_merchant %>%
  gt() %>%
  tab_header(
    title = "Merchant Failure Rates",
    subtitle = "Comparison of Failure Rates Before and During Anomaly"
  ) %>%
  cols_label(
    Merchant = "Merchant",
    Failure_rate_Pre = "Failure Rate Before Anomaly",
    Failure_Rate_Anamoly = "Failure Rate During Anomaly",
    Failure_Rate_Difference = "Difference in Failure Rate"
  ) %>%
  fmt_number(
    columns = c(Failure_rate_Pre, Failure_Rate_Anamoly, Failure_Rate_Difference),
    decimals = 4
  ) %>%
  tab_style(
    style = cell_text(weight = "bold"),
    locations = cells_column_labels(everything())
  ) %>%
  tab_options(
    table.font.size = "small",
    table.width = pct(80),
    heading.align = "center"
  ) %>%
  data_color(
    columns = Failure_Rate_Difference,
    colors = scales::col_numeric(
      palette = c("lightblue", "red"),
      domain = NULL
    )
  )

gtsave(
  gt_table, 
  "~/Desktop/DS_DA_Projects/Anamoly_Detection/ReadMe_files/figure-gfm/gt_table_image.png"
)

knitr::include_graphics(
  "~/Desktop/DS_DA_Projects/Anamoly_Detection/ReadMe_files/figure-gfm/gt_table_image.png"
)

```

All of the merchants were effected except UrbanClap.

Now that we found the problem and who was impacted, we investigate further with the following questions.

1.) Were there indicators that predicted this event so we can catch it before it happens next time?

2.) If this could not have been predicted, how can we make sure we detect it as soon as possible if it does happen again?

First, we separate the data that contains the anamoly from the normal data and produce the following plots to compare the bias and variances of the curves to the mean failure rate of the normal data across the entire data set.

Then, to produce a fair comparison, we limit the sample size of the normal data we plot to be equal to the sample size of the anomaly's data.

anomaly data:

```{r}
paytm_subset <- data[
  (data[, 5] %in% c("PAYTM_V2")) & 
  (data[, 6] %in% c("UPI_COLLECT")) & 
  (data[, 4] == "UPI"), 
]

unique_hours <- unique(data$hr)
unique_hours <- sort(unique_hours)

t <- aggregate(paytm_subset$t, by = list(paytm_subset$hr), sum)
s <- aggregate(paytm_subset$s, by = list(paytm_subset$hr), sum)
f <- t[, 2] - s[, 2]

proportion_subset <- f / t[, 2] * 100

transactions_subset <- t
```

all normal data:

```{r}
paytm_compliment <- data[!(rownames(data) %in% rownames(paytm_subset)), ]
t <- aggregate(paytm_compliment$t, by = list(paytm_compliment$hr), sum)
s <- aggregate(paytm_compliment$s, by = list(paytm_compliment$hr), sum)
f <- t[, 2] - s[, 2]

proportion_compliment <- f / t[, 2] * 100

transactions_compliment <- t
```

normal data but roughly equal number of transactions as the anomalous subset selected at random:

```{r}
paytm_compliment_sample <- paytm_compliment[sample(nrow(paytm_compliment), 1200), ]

t <- aggregate(paytm_compliment_sample$t, by = list(paytm_compliment_sample$hr), sum)

s <- aggregate(paytm_compliment_sample$s, by = list(paytm_compliment_sample$hr), sum)

f <- t[, 2] - s[, 2]

proportion_compliment_sample <- f / t[, 2] * 100
 
compliment_sample_sizes <- t
```

plot of the anomalous data and normal data of equal sample size to the anomaly:

```{r echo=FALSE}
hours <- seq(1, 72, 1)
wide <- as.data.frame(cbind(hours, proportion_subset, proportion_compliment_sample))

wide$mean_failure_rate <- 34.6  

long <- melt(wide, id.vars = "hours", measure.vars = c("proportion_subset", "proportion_compliment_sample", "mean_failure_rate"),
             variable.name = "percentage_failure", value.name = "value")

ggplot(data = long, aes(x = hours, y = value, group = percentage_failure, color = percentage_failure)) +
  geom_smooth(data = long[long$percentage_failure != "mean_failure_rate", ], se = FALSE) +  # smooth line for other data
  geom_line(data = long[long$percentage_failure == "mean_failure_rate", ], linetype = "dashed") +  # dashed line for mean failure rate
  labs(title = "Smoothed Failure Percentage Curve", y = "Percentage", color = "Legend") +
  scale_color_manual(values = c("blue", "green", "red"),
                     labels = c("Anomalous Data", "Non-Anomalous Data Sample", "Mean Failure Rate")) +
  theme_minimal()
```

In the hours before the anomaly occurs, the anomalous data has a better fit to the mean failure rate of the normal data than the normal data of equal sample size does.

```{r echo=FALSE}
ggplot(data = long, aes(x = hours, y = value, group = percentage_failure, color = percentage_failure)) +
  geom_line(data = long[long$percentage_failure != "mean_failure_rate", ], se = FALSE) +  # smooth line for other data
  geom_line(data = long[long$percentage_failure == "mean_failure_rate", ], linetype = "dashed") +  # dashed line for mean failure rate
  labs(title = "Smoothed Failure Percentage Curve", y = "Percentage", color = "Legend") +
  scale_color_manual(values = c("blue", "green", "red"),
                     labels = c("Anomalous Data", "Non-Anomalous Data Sample", "Mean Failure Rate")) +
  theme_minimal()

```

Plotting without a curve smoother, you can see that before the anomaly, the variance of the normal data sample isn't any better than the anomaly data.

Confirming the result are the following:

This gif is of the distribution of the anomalous data failure rate over a shifting 18 hour window clearly displaying the mean shift we observe in the anomaly.

![](ReadMe_files/figure-gfm/density_animation_high_quality.gif)

As well, here is the density curve of the anomalous data and normal data of equal sample size, notice how the anomalous data is synthetic by it's perfect distribution curve.

```{r echo=FALSE}
library(plotly)
#[Density Plot Animation](ReadMe_files/figure-gfm/density_animation_high_quality.gif)
ggplot(data = wide, aes(proportion_subset)) + 
  geom_density(data=wide,aes(proportion_compliment_sample,fill="red",alpha=0.20)) + 
  geom_density(fill = "blue", alpha = 0.20) + 
  theme_minimal() + 
  labs(
    title = "Anomalous Data Failure Rate Density", 
    x = "Failure Rate", 
    y = "Density"
  )

```

What all of these plots indicate is that the anomalous data before the anomaly event was no different than the rest of the data, something happened outside of the data set to cause the spike in failure rate. This is evident in the density curve of the anomaly, containing a second node displaying the failure rate during the anomaly as it's own district distribution.

So now that we know that this failure rate event could not had been predicted. How can we monitor the data to see an event occurring before any of out clients do.

The following is a interactive dashboard displaying the failure rates and transaction volume of each combination of categories within the data set. These visualizations upon your inspection will clearly reveal the normal and abnormal patterns in the data which we have discovered.

**Right click to open in a new tab.**

```{r, include = FALSE}
library(shiny)

#shiny::runApp("/Users/jacobrichards/Desktop/DS_DA_Projects/Anamoly_Detection/shiny")


#You can access the live Shiny app here: [Shiny App on shinyapps.io](https://jacob-j-richards.shinyapps.io/shiny/)

```

[shiny dashboard of failure rate and transaction](https://jacob-j-richards.shinyapps.io/shiny/ "open in a new tab")
