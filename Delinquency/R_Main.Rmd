---
title: "Untitled"
output: html_document
date: "2024-11-23"
---

# Instruction

The panel data-set contains commercial customers' financial information and days past due indicator from 2000 to 2020. The goal is to build a binary classifier to predict customers 90+ days past due **(90+DPD)** probability.

```{r}
library(tidyverse)
train <- read.csv(file="FITB_train.csv",header=TRUE)
test <- read.csv(file="FITB_test.csv",header=TRUE)
train_unmodified <- train
```

2.  Winsorize **"feature_3"** by limiting the extreme values to 1 percentile (left tail) and 99 percentile (right tail) on the training set. Name it **"feature_3_winsor"**. Make appropriate treatments on the testing set.

```{r}
library(ggplot2)
ggplot(data=train,aes(feature_3)) + geom_density()
ggplot(data=test,aes(feature_3)) + geom_density()
```

Clearly this is a problem. Let's cut out the 1% tails on both ends on the training set. Leaving the test data alone since we want to see how the model does on sub-optimal inputs.

```{r}
train$key <- row.names(train)
feature_3_winsor <- data.frame(feature_3 = train$feature_3, key = row.names(train))
feature_3_winsor_clean <- na.omit(feature_3_winsor)

feature_3_winsor_clean <- feature_3_winsor_clean %>%
  mutate(z_score = (feature_3 - mean(feature_3)) / sd(feature_3),percentile = ecdf(feature_3)(feature_3) * 100)

feature_3_winsor_df <- feature_3_winsor_clean[!(feature_3_winsor_clean[, 4] < 1 | feature_3_winsor_clean[, 4] > 99), ]

non_matching_keys <- anti_join(train, feature_3_winsor_df, by = "key")$key

train <- train %>% mutate(feature_3 = ifelse(key %in% non_matching_keys, NA, feature_3))

colnames(train)[3] <- "feature_3_winsor"
```

```{r}
ggplot(data=train,aes(feature_3_winsor)) + geom_density()
```

3.  Identify missing values for **"feature_3_winsor"**. Then, impute the missings with median value on the training set. Name the new feature **"feature_3_impute"**. Make appropriate treatments on the testing set.

```{r}
train[is.na(train[,3]),3] <- median(feature_3_winsor_clean$feature_3)

colnames(train)[3] <- "feature_3_impute"

test[is.na(test[,3]),3] <- median(feature_3_winsor_clean$feature_3)
colnames(test)[3] <- "feature_3_impute"
```

4.  Identify missing values for **"feature_2"**. Then, for each **"id"**, impute the missings with the value from previous year, if not available, use the value from next year. Name the new feature **"feature_2_impute"**. Make appropriate treatments on the testing set.

This time will do the same thing for training and testing data

```{r}
train$date <- format(as.Date(train$date, format = "%Y-%m-%d"), "%Y")
library(dplyr)

train <- train %>%
  arrange(id, date) %>% # Sort by id and date
  group_by(id) %>%
  mutate(feature_2 = ifelse(is.na(feature_2),
                            lead(feature_2, order_by = date), # Try next year
                            feature_2)) %>%
  mutate(feature_2 = ifelse(is.na(feature_2),
                            lag(feature_2, order_by = date), # Try previous year
                            feature_2))


colnames(train)[2] <- "feature_2_impute"




test <- test %>%
  arrange(id, date) %>% # Sort by id and date
  group_by(id) %>%
  mutate(feature_2 = ifelse(is.na(feature_2),
                            lead(feature_2, order_by = date), # Try next year
                            feature_2)) %>%
  mutate(feature_2 = ifelse(is.na(feature_2),
                            lag(feature_2, order_by = date), # Try previous year
                            feature_2))



colnames(test)[2] <- "feature_2_impute"


train <- na.omit(train)
test <- na.omit(test)


cleaned_unnormalized_train <- train

```

5.  Standardize **"feature_1"**, **"feature_2_impute"**, **"feature_3_impute"**, **"feature_4"** for the training set. Make appropriate treatments on the testing set. Name the features **"feature_1_standard"**, **"feature_2_standard"**, **"feature_3_standard"**, **"feature_4_standard"**

standardize by z-score for both testing and training

```{r}
library(ggplot2)
ggplot(data=train,aes(x=feature_1)) + geom_density() + geom_density(data=train,aes(x=feature_2_impute)) + geom_density(data=train,aes(x=feature_3_impute))
train
```

```{r}
library(dplyr)
train <- train %>%
  mutate(across(c(feature_1, feature_2_impute, feature_3_impute, feature_4), 
                ~ (.x - mean(.x, na.rm = TRUE)) / sd(.x, na.rm = TRUE)))
train
ggplot(data=train,aes(x=feature_1)) + geom_density() + geom_density(data=train,aes(x=feature_2_impute)) + geom_density(data=train,aes(x=feature_3_impute))

colnames(train) <- c("feature_1_standard","feature_2_standard","feature_3_standard","feature_4_standard","id","date","y","key")


test <- test %>%
  mutate(across(c(feature_1, feature_2_impute, feature_3_impute, feature_4), 
                ~ (.x - mean(.x, na.rm = TRUE)) / sd(.x, na.rm = TRUE)))

ggplot(data=test,aes(x=feature_1,color="blue")) + geom_density() + geom_density(data=test,aes(x=feature_2_impute,color="red")) + geom_density(data=test,aes(x=feature_3_impute,color="green"))


colnames(test) <- c("feature_1_standard","feature_2_standard","feature_3_standard","feature_4_standard","id","date","y")


write.csv(file="final_train.csv",train)
```

6.  Build a logistic regression on the training set to predict **'y'** being **(90+DPD)** using **"feature_1_standard"**, **"feature_2_standard"**, **"feature_3_standard"**, **"feature_4_standard"** as independent variables.

```{r}
library(nnet)
train$y <- as.numeric(as.character(factor(train$y, levels = c("90+DPD", "active"), labels = c(1, 0))))
delinquency_model <- multinom(y ~ feature_1_standard + feature_2_standard + feature_3_standard + feature_4_standard, data=train,family=binomial())
summary(delinquency_model)
```

7.  Report AUC on the **testing set**.

```{r}
    library(pROC)
    library(ggplot2)
    test$predicted_y <- predict(delinquency_model, newdata = test, type = "class")
    test$y_numeric <- as.numeric(as.character(factor(test$y, levels = c("90+DPD", "active"), labels = c(1, 0))))
    test$predicted_probs <- predict(delinquency_model, newdata = test, type = "probs")
    test$probs <- round(test$predicted_probs * 100, 1)
    test
    
    roc_curve <- roc(response = test$y_numeric, predictor = test$predicted_probs)
    plot(roc_curve, main = "ROC Curve for Multinomial Logistic Regression", col = "blue", lwd = 2)

    print(paste("AUC:", auc(roc_curve)))
  
    roc_metrics <- coords(roc_curve, x = "all", ret = c("threshold", "sensitivity", "specificity"))
    

    roc_metrics$threshold <- as.numeric(roc_metrics$threshold)  # Ensure threshold is numeric
    ggplot(roc_metrics, aes(x = threshold)) +
    geom_line(aes(y = sensitivity, color = "Sensitivity")) +
    geom_line(aes(y = specificity, color = "Specificity")) +
    labs(title = "Sensitivity and Specificity vs. Threshold",
    x = "Threshold", y = "Metric Value") +
    scale_color_manual(name = "Metrics", values = c("Sensitivity" = "red", "Specificity" = "blue"))
    
    cat("optimal threshold",roc_metrics$threshold[which.min(abs(roc_metrics$sensitivity - roc_metrics$specificity))])
```

Specificity is the proportion of negative outcomes correctly identified by the model (what proportion of people who did not go delinquent on payments were correctly identified by the model)

Sensitivity is the proportion of positive outcomes correctly identified by the model ( what proportion of delinquencies were caught by the model)

The Threshold which maximizes "catches" (correct prediction of delinquent payments) and minimizes "false alarms" (predicting delinquent payments which are paid on time).

8.  Report confusion matrix on the **testing set**.

```{r}
test$predicted_class <- ifelse(test$predicted_probs >= roc_metrics$threshold[which.min(abs(roc_metrics$sensitivity - roc_metrics$specificity))], 1, 0)

library(caret)
conf_matrix <- confusionMatrix(
  factor(test$predicted_class, levels = c(0, 1)),
  factor(test$y_numeric, levels = c(0, 1)))

confusion_table <- as.data.frame.matrix(conf_matrix$table)

rownames(confusion_table) <- c("Actual: Non-delinquent", "Actual: Delinquent")
colnames(confusion_table) <- c("Predicted: Non-delinquent", "Predicted: Delinquent")

print("Confusion Matrix:")
print(confusion_table)

true_positives <- confusion_table[2, 2]  # Delinquent correctly classified
false_positives <- confusion_table[1, 2] # Non-delinquent misclassified as delinquent
true_negatives <- confusion_table[1, 1]  # Non-delinquent correctly classified
false_negatives <- confusion_table[2, 1] # Delinquent missed


```

9.  Identify multicollinearity in the model.

```{r}

library(car)

# Create the design matrix (without the response variable)
X <- model.matrix(~ feature_1 + feature_2_impute + feature_3_impute + feature_4, data=cleaned_unnormalized_train)

# Calculate the VIF for each predictor
vif_values <- diag(solve(cor(X[, -1])))  # Exclude intercept column
names(vif_values) <- colnames(X)[-1]    # Assign names
print(vif_values)
library(corrplot)




cor_matrix <- cor(cleaned_unnormalized_train[, c("feature_1", "feature_2_impute", "feature_3_impute", "feature_4")])

corrplot(cor_matrix, 
         method = "color",        # Use colored squares
         col = colorRampPalette(c("white", "red"))(200),  # White to red gradient
         type = "upper",          # Show only the upper triangle
         tl.col = "black",        # Text labels in black
         tl.srt = 45,             # Rotate labels
         addCoef.col = "black",   # Display correlation coefficients in black
         number.cex = 0.8)        # Adjust size of numbers



```

There is mulit-collinearlity in that features 1, 2, and 3 as they have a moderate coloration coefficient of .41.
