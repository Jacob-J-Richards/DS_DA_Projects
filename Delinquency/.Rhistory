test$predicted_y <- predict(delinquency_model, newdata = test, type = "class")
test$y_numeric <- as.numeric(as.character(factor(test$y, levels = c("90+DPD", "active"), labels = c(1, 0))))
test$predicted_probs <- predict(delinquency_model, newdata = test, type = "probs")
test$probs <- round(test$predicted_probs * 100, 1)
test
roc_curve <- roc(response = test$y_numeric, predictor = test$predicted_probs)
plot(roc_curve, main = "ROC Curve for Multinomial Logistic Regression", col = "blue", lwd = 2)
print(paste("AUC:", auc(roc_curve)))
roc_metrics <- coords(roc_curve, x = "all", ret = c("threshold", "sensitivity", "specificity"))
roc_metrics$threshold <- as.numeric(roc_metrics$threshold)  # Ensure threshold is numeric
ggplot(roc_metrics, aes(x = threshold)) +
geom_line(aes(y = sensitivity, color = "Sensitivity")) +
geom_line(aes(y = specificity, color = "Specificity")) +
labs(title = "Sensitivity and Specificity vs. Threshold",
x = "Threshold", y = "Metric Value") +
scale_color_manual(name = "Metrics", values = c("Sensitivity" = "red", "Specificity" = "blue"))
cat("optimal threshold",roc_metrics$threshold[which.min(abs(roc_metrics$sensitivity - roc_metrics$specificity))])
test$predicted_class <- ifelse(test$predicted_probs >= roc_metrics$threshold[which.min(abs(roc_metrics$sensitivity - roc_metrics$specificity))], 1, 0)
library(caret)
conf_matrix <- confusionMatrix(
factor(test$predicted_class, levels = c(0, 1)),
factor(test$y_numeric, levels = c(0, 1)))
confusion_table <- as.data.frame.matrix(conf_matrix$table)
rownames(confusion_table) <- c("Actual: Non-delinquent", "Actual: Delinquent")
colnames(confusion_table) <- c("Predicted: Non-delinquent", "Predicted: Delinquent")
print("Confusion Matrix:")
print(confusion_table)
true_positives <- confusion_table[2, 2]  # Delinquent correctly classified
false_positives <- confusion_table[1, 2] # Non-delinquent misclassified as delinquent
true_negatives <- confusion_table[1, 1]  # Non-delinquent correctly classified
false_negatives <- confusion_table[2, 1] # Delinquent missed
library(car)
# Create the design matrix (without the response variable)
X <- model.matrix(~ feature_1 + feature_2_impute + feature_3_impute + feature_4, data=cleaned_unnormalized_train)
# Calculate the VIF for each predictor
vif_values <- diag(solve(cor(X[, -1])))  # Exclude intercept column
names(vif_values) <- colnames(X)[-1]    # Assign names
print(vif_values)
library(corrplot)
# Calculate the correlation matrix
cor_matrix <- cor(cleaned_unnormalized_train[, c("feature_1", "feature_2_impute", "feature_3_impute", "feature_4")])
# Visualize the correlation matrix with actual values and white-to-red gradient
corrplot(cor_matrix,
method = "color",        # Use colored squares
col = colorRampPalette(c("white", "red"))(200),  # White to red gradient
type = "upper",          # Show only the upper triangle
tl.col = "black",        # Text labels in black
tl.srt = 45,             # Rotate labels
addCoef.col = "black",   # Display correlation coefficients in black
number.cex = 0.8)        # Adjust size of numbers
library(tidyverse)
train <- read.csv(file="FITB_train.csv",header=TRUE)
test <- read.csv(file="FITB_test.csv",header=TRUE)
train_unmodified <- train
library(ggplot2)
ggplot(data=train,aes(feature_3)) + geom_density()
ggplot(data=test,aes(feature_3)) + geom_density()
train$key <- row.names(train)
feature_3_winsor <- data.frame(feature_3 = train$feature_3, key = row.names(train))
feature_3_winsor_clean <- na.omit(feature_3_winsor)
feature_3_winsor_clean <- feature_3_winsor_clean %>%
mutate(z_score = (feature_3 - mean(feature_3)) / sd(feature_3),percentile = ecdf(feature_3)(feature_3) * 100)
feature_3_winsor_df <- feature_3_winsor_clean[!(feature_3_winsor_clean[, 4] < 1 | feature_3_winsor_clean[, 4] > 99), ]
non_matching_keys <- anti_join(train, feature_3_winsor_df, by = "key")$key
train <- train %>% mutate(feature_3 = ifelse(key %in% non_matching_keys, NA, feature_3))
colnames(train)[3] <- "feature_3_winsor"
ggplot(data=train,aes(feature_3_winsor)) + geom_density()
train[is.na(train[,3]),3] <- median(feature_3_winsor_clean$feature_3)
colnames(train)[3] <- "feature_3_impute"
test[is.na(test[,3]),3] <- median(feature_3_winsor_clean$feature_3)
colnames(test)[3] <- "feature_3_impute"
train$date <- format(as.Date(train$date, format = "%Y-%m-%d"), "%Y")
library(dplyr)
train <- train %>%
arrange(id, date) %>% # Sort by id and date
group_by(id) %>%
mutate(feature_2 = ifelse(is.na(feature_2),
lead(feature_2, order_by = date), # Try next year
feature_2)) %>%
mutate(feature_2 = ifelse(is.na(feature_2),
lag(feature_2, order_by = date), # Try previous year
feature_2))
colnames(train)[2] <- "feature_2_impute"
test <- test %>%
arrange(id, date) %>% # Sort by id and date
group_by(id) %>%
mutate(feature_2 = ifelse(is.na(feature_2),
lead(feature_2, order_by = date), # Try next year
feature_2)) %>%
mutate(feature_2 = ifelse(is.na(feature_2),
lag(feature_2, order_by = date), # Try previous year
feature_2))
colnames(test)[2] <- "feature_2_impute"
cleaned_unnormalized_train <- train
library(ggplot2)
ggplot(data=train,aes(x=feature_1)) + geom_density() + geom_density(data=train,aes(x=feature_2_impute)) + geom_density(data=train,aes(x=feature_3_impute))
train
library(dplyr)
train <- train %>%
mutate(across(c(feature_1, feature_2_impute, feature_3_impute, feature_4),
~ (.x - mean(.x, na.rm = TRUE)) / sd(.x, na.rm = TRUE)))
train
ggplot(data=train,aes(x=feature_1)) + geom_density() + geom_density(data=train,aes(x=feature_2_impute)) + geom_density(data=train,aes(x=feature_3_impute))
colnames(train) <- c("feature_1_standard","feature_2_standard","feature_3_standard","feature_4_standard","id","date","y","key")
test <- test %>%
mutate(across(c(feature_1, feature_2_impute, feature_3_impute, feature_4),
~ (.x - mean(.x, na.rm = TRUE)) / sd(.x, na.rm = TRUE)))
ggplot(data=test,aes(x=feature_1,color="blue")) + geom_density() + geom_density(data=test,aes(x=feature_2_impute,color="red")) + geom_density(data=test,aes(x=feature_3_impute,color="green"))
colnames(test) <- c("feature_1_standard","feature_2_standard","feature_3_standard","feature_4_standard","id","date","y")
write.csv(file="final_train.csv",train)
library(nnet)
train$y <- as.numeric(as.character(factor(train$y, levels = c("90+DPD", "active"), labels = c(1, 0))))
delinquency_model <- multinom(y ~ feature_1_standard + feature_2_standard + feature_3_standard + feature_4_standard, data=train,family=binomial())
summary(delinquency_model)
library(pROC)
library(ggplot2)
test$predicted_y <- predict(delinquency_model, newdata = test, type = "class")
test$y_numeric <- as.numeric(as.character(factor(test$y, levels = c("90+DPD", "active"), labels = c(1, 0))))
test$predicted_probs <- predict(delinquency_model, newdata = test, type = "probs")
test$probs <- round(test$predicted_probs * 100, 1)
test
roc_curve <- roc(response = test$y_numeric, predictor = test$predicted_probs)
plot(roc_curve, main = "ROC Curve for Multinomial Logistic Regression", col = "blue", lwd = 2)
print(paste("AUC:", auc(roc_curve)))
roc_metrics <- coords(roc_curve, x = "all", ret = c("threshold", "sensitivity", "specificity"))
roc_metrics$threshold <- as.numeric(roc_metrics$threshold)  # Ensure threshold is numeric
ggplot(roc_metrics, aes(x = threshold)) +
geom_line(aes(y = sensitivity, color = "Sensitivity")) +
geom_line(aes(y = specificity, color = "Specificity")) +
labs(title = "Sensitivity and Specificity vs. Threshold",
x = "Threshold", y = "Metric Value") +
scale_color_manual(name = "Metrics", values = c("Sensitivity" = "red", "Specificity" = "blue"))
cat("optimal threshold",roc_metrics$threshold[which.min(abs(roc_metrics$sensitivity - roc_metrics$specificity))])
test$predicted_class <- ifelse(test$predicted_probs >= roc_metrics$threshold[which.min(abs(roc_metrics$sensitivity - roc_metrics$specificity))], 1, 0)
library(caret)
conf_matrix <- confusionMatrix(
factor(test$predicted_class, levels = c(0, 1)),
factor(test$y_numeric, levels = c(0, 1)))
confusion_table <- as.data.frame.matrix(conf_matrix$table)
rownames(confusion_table) <- c("Actual: Non-delinquent", "Actual: Delinquent")
colnames(confusion_table) <- c("Predicted: Non-delinquent", "Predicted: Delinquent")
print("Confusion Matrix:")
print(confusion_table)
true_positives <- confusion_table[2, 2]  # Delinquent correctly classified
false_positives <- confusion_table[1, 2] # Non-delinquent misclassified as delinquent
true_negatives <- confusion_table[1, 1]  # Non-delinquent correctly classified
false_negatives <- confusion_table[2, 1] # Delinquent missed
library(car)
# Create the design matrix (without the response variable)
X <- model.matrix(~ feature_1 + feature_2_impute + feature_3_impute + feature_4, data=cleaned_unnormalized_train)
# Calculate the VIF for each predictor
vif_values <- diag(solve(cor(X[, -1])))  # Exclude intercept column
names(vif_values) <- colnames(X)[-1]    # Assign names
print(vif_values)
library(corrplot)
cor_matrix <- cor(cleaned_unnormalized_train[, c("feature_1", "feature_2_impute", "feature_3_impute", "feature_4")])
corrplot(cor_matrix,
method = "color",        # Use colored squares
col = colorRampPalette(c("white", "red"))(200),  # White to red gradient
type = "upper",          # Show only the upper triangle
tl.col = "black",        # Text labels in black
tl.srt = 45,             # Rotate labels
addCoef.col = "black",   # Display correlation coefficients in black
number.cex = 0.8)        # Adjust size of numbers
library(tidyverse)
train <- read.csv(file="FITB_train.csv",header=TRUE)
test <- read.csv(file="FITB_test.csv",header=TRUE)
train_unmodified <- train
library(ggplot2)
ggplot(data=train,aes(feature_3)) + geom_density()
ggplot(data=test,aes(feature_3)) + geom_density()
train$key <- row.names(train)
feature_3_winsor <- data.frame(feature_3 = train$feature_3, key = row.names(train))
feature_3_winsor_clean <- na.omit(feature_3_winsor)
feature_3_winsor_clean <- feature_3_winsor_clean %>%
mutate(z_score = (feature_3 - mean(feature_3)) / sd(feature_3),percentile = ecdf(feature_3)(feature_3) * 100)
feature_3_winsor_df <- feature_3_winsor_clean[!(feature_3_winsor_clean[, 4] < 1 | feature_3_winsor_clean[, 4] > 99), ]
non_matching_keys <- anti_join(train, feature_3_winsor_df, by = "key")$key
train <- train %>% mutate(feature_3 = ifelse(key %in% non_matching_keys, NA, feature_3))
colnames(train)[3] <- "feature_3_winsor"
ggplot(data=train,aes(feature_3_winsor)) + geom_density()
train[is.na(train[,3]),3] <- median(feature_3_winsor_clean$feature_3)
colnames(train)[3] <- "feature_3_impute"
test[is.na(test[,3]),3] <- median(feature_3_winsor_clean$feature_3)
colnames(test)[3] <- "feature_3_impute"
train$date <- format(as.Date(train$date, format = "%Y-%m-%d"), "%Y")
library(dplyr)
train <- train %>%
arrange(id, date) %>% # Sort by id and date
group_by(id) %>%
mutate(feature_2 = ifelse(is.na(feature_2),
lead(feature_2, order_by = date), # Try next year
feature_2)) %>%
mutate(feature_2 = ifelse(is.na(feature_2),
lag(feature_2, order_by = date), # Try previous year
feature_2))
colnames(train)[2] <- "feature_2_impute"
cleaned_unnormalized_train <- train
test <- test %>%
arrange(id, date) %>% # Sort by id and date
group_by(id) %>%
mutate(feature_2 = ifelse(is.na(feature_2),
lead(feature_2, order_by = date), # Try next year
feature_2)) %>%
mutate(feature_2 = ifelse(is.na(feature_2),
lag(feature_2, order_by = date), # Try previous year
feature_2))
colnames(test)[2] <- "feature_2_impute"
library(ggplot2)
ggplot(data=train,aes(x=feature_1)) + geom_density() + geom_density(data=train,aes(x=feature_2_impute)) + geom_density(data=train,aes(x=feature_3_impute))
train
library(dplyr)
train <- train %>%
mutate(across(c(feature_1, feature_2_impute, feature_3_impute, feature_4),
~ (.x - mean(.x, na.rm = TRUE)) / sd(.x, na.rm = TRUE)))
train
ggplot(data=train,aes(x=feature_1)) + geom_density() + geom_density(data=train,aes(x=feature_2_impute)) + geom_density(data=train,aes(x=feature_3_impute))
colnames(train) <- c("feature_1_standard","feature_2_standard","feature_3_standard","feature_4_standard","id","date","y","key")
test <- test %>%
mutate(across(c(feature_1, feature_2_impute, feature_3_impute, feature_4),
~ (.x - mean(.x, na.rm = TRUE)) / sd(.x, na.rm = TRUE)))
ggplot(data=test,aes(x=feature_1,color="blue")) + geom_density() + geom_density(data=test,aes(x=feature_2_impute,color="red")) + geom_density(data=test,aes(x=feature_3_impute,color="green"))
colnames(test) <- c("feature_1_standard","feature_2_standard","feature_3_standard","feature_4_standard","id","date","y")
write.csv(file="final_train.csv",train)
library(nnet)
train$y <- as.numeric(as.character(factor(train$y, levels = c("90+DPD", "active"), labels = c(1, 0))))
delinquency_model <- multinom(y ~ feature_1_standard + feature_2_standard + feature_3_standard + feature_4_standard, data=train,family=binomial())
summary(delinquency_model)
library(pROC)
library(ggplot2)
test$predicted_y <- predict(delinquency_model, newdata = test, type = "class")
test$y_numeric <- as.numeric(as.character(factor(test$y, levels = c("90+DPD", "active"), labels = c(1, 0))))
test$predicted_probs <- predict(delinquency_model, newdata = test, type = "probs")
test$probs <- round(test$predicted_probs * 100, 1)
test
roc_curve <- roc(response = test$y_numeric, predictor = test$predicted_probs)
plot(roc_curve, main = "ROC Curve for Multinomial Logistic Regression", col = "blue", lwd = 2)
print(paste("AUC:", auc(roc_curve)))
roc_metrics <- coords(roc_curve, x = "all", ret = c("threshold", "sensitivity", "specificity"))
roc_metrics$threshold <- as.numeric(roc_metrics$threshold)  # Ensure threshold is numeric
ggplot(roc_metrics, aes(x = threshold)) +
geom_line(aes(y = sensitivity, color = "Sensitivity")) +
geom_line(aes(y = specificity, color = "Specificity")) +
labs(title = "Sensitivity and Specificity vs. Threshold",
x = "Threshold", y = "Metric Value") +
scale_color_manual(name = "Metrics", values = c("Sensitivity" = "red", "Specificity" = "blue"))
cat("optimal threshold",roc_metrics$threshold[which.min(abs(roc_metrics$sensitivity - roc_metrics$specificity))])
test$predicted_class <- ifelse(test$predicted_probs >= roc_metrics$threshold[which.min(abs(roc_metrics$sensitivity - roc_metrics$specificity))], 1, 0)
library(caret)
conf_matrix <- confusionMatrix(
factor(test$predicted_class, levels = c(0, 1)),
factor(test$y_numeric, levels = c(0, 1)))
confusion_table <- as.data.frame.matrix(conf_matrix$table)
rownames(confusion_table) <- c("Actual: Non-delinquent", "Actual: Delinquent")
colnames(confusion_table) <- c("Predicted: Non-delinquent", "Predicted: Delinquent")
print("Confusion Matrix:")
print(confusion_table)
true_positives <- confusion_table[2, 2]  # Delinquent correctly classified
false_positives <- confusion_table[1, 2] # Non-delinquent misclassified as delinquent
true_negatives <- confusion_table[1, 1]  # Non-delinquent correctly classified
false_negatives <- confusion_table[2, 1] # Delinquent missed
library(car)
# Create the design matrix (without the response variable)
X <- model.matrix(~ feature_1 + feature_2_impute + feature_3_impute + feature_4, data=cleaned_unnormalized_train)
# Calculate the VIF for each predictor
vif_values <- diag(solve(cor(X[, -1])))  # Exclude intercept column
names(vif_values) <- colnames(X)[-1]    # Assign names
print(vif_values)
library(corrplot)
cor_matrix <- cor(cleaned_unnormalized_train[, c("feature_1", "feature_2_impute", "feature_3_impute", "feature_4")])
corrplot(cor_matrix,
method = "color",        # Use colored squares
col = colorRampPalette(c("white", "red"))(200),  # White to red gradient
type = "upper",          # Show only the upper triangle
tl.col = "black",        # Text labels in black
tl.srt = 45,             # Rotate labels
addCoef.col = "black",   # Display correlation coefficients in black
number.cex = 0.8)        # Adjust size of numbers
library(tidyverse)
train <- read.csv(file="FITB_train.csv",header=TRUE)
test <- read.csv(file="FITB_test.csv",header=TRUE)
train_unmodified <- train
library(ggplot2)
ggplot(data=train,aes(feature_3)) + geom_density()
ggplot(data=test,aes(feature_3)) + geom_density()
train$key <- row.names(train)
feature_3_winsor <- data.frame(feature_3 = train$feature_3, key = row.names(train))
feature_3_winsor_clean <- na.omit(feature_3_winsor)
feature_3_winsor_clean <- feature_3_winsor_clean %>%
mutate(z_score = (feature_3 - mean(feature_3)) / sd(feature_3),percentile = ecdf(feature_3)(feature_3) * 100)
feature_3_winsor_df <- feature_3_winsor_clean[!(feature_3_winsor_clean[, 4] < 1 | feature_3_winsor_clean[, 4] > 99), ]
non_matching_keys <- anti_join(train, feature_3_winsor_df, by = "key")$key
train <- train %>% mutate(feature_3 = ifelse(key %in% non_matching_keys, NA, feature_3))
colnames(train)[3] <- "feature_3_winsor"
ggplot(data=train,aes(feature_3_winsor)) + geom_density()
train[is.na(train[,3]),3] <- median(feature_3_winsor_clean$feature_3)
colnames(train)[3] <- "feature_3_impute"
test[is.na(test[,3]),3] <- median(feature_3_winsor_clean$feature_3)
colnames(test)[3] <- "feature_3_impute"
train$date <- format(as.Date(train$date, format = "%Y-%m-%d"), "%Y")
library(dplyr)
train <- train %>%
arrange(id, date) %>% # Sort by id and date
group_by(id) %>%
mutate(feature_2 = ifelse(is.na(feature_2),
lead(feature_2, order_by = date), # Try next year
feature_2)) %>%
mutate(feature_2 = ifelse(is.na(feature_2),
lag(feature_2, order_by = date), # Try previous year
feature_2))
# Example: Count NA values in a specific column
num_na <- sum(is.na(train$feature_2))
# Print the result
print(num_na)
colnames(train)[2] <- "feature_2_impute"
cleaned_unnormalized_train <- train
test <- test %>%
arrange(id, date) %>% # Sort by id and date
group_by(id) %>%
mutate(feature_2 = ifelse(is.na(feature_2),
lead(feature_2, order_by = date), # Try next year
feature_2)) %>%
mutate(feature_2 = ifelse(is.na(feature_2),
lag(feature_2, order_by = date), # Try previous year
feature_2))
colnames(test)[2] <- "feature_2_impute"
library(ggplot2)
ggplot(data=train,aes(x=feature_1)) + geom_density() + geom_density(data=train,aes(x=feature_2_impute)) + geom_density(data=train,aes(x=feature_3_impute))
train
library(dplyr)
train <- train %>%
mutate(across(c(feature_1, feature_2_impute, feature_3_impute, feature_4),
~ (.x - mean(.x, na.rm = TRUE)) / sd(.x, na.rm = TRUE)))
train
ggplot(data=train,aes(x=feature_1)) + geom_density() + geom_density(data=train,aes(x=feature_2_impute)) + geom_density(data=train,aes(x=feature_3_impute))
colnames(train) <- c("feature_1_standard","feature_2_standard","feature_3_standard","feature_4_standard","id","date","y","key")
test <- test %>%
mutate(across(c(feature_1, feature_2_impute, feature_3_impute, feature_4),
~ (.x - mean(.x, na.rm = TRUE)) / sd(.x, na.rm = TRUE)))
ggplot(data=test,aes(x=feature_1,color="blue")) + geom_density() + geom_density(data=test,aes(x=feature_2_impute,color="red")) + geom_density(data=test,aes(x=feature_3_impute,color="green"))
colnames(test) <- c("feature_1_standard","feature_2_standard","feature_3_standard","feature_4_standard","id","date","y")
write.csv(file="final_train.csv",train)
library(nnet)
train$y <- as.numeric(as.character(factor(train$y, levels = c("90+DPD", "active"), labels = c(1, 0))))
delinquency_model <- multinom(y ~ feature_1_standard + feature_2_standard + feature_3_standard + feature_4_standard, data=train,family=binomial())
summary(delinquency_model)
library(pROC)
library(ggplot2)
test$predicted_y <- predict(delinquency_model, newdata = test, type = "class")
test$y_numeric <- as.numeric(as.character(factor(test$y, levels = c("90+DPD", "active"), labels = c(1, 0))))
test$predicted_probs <- predict(delinquency_model, newdata = test, type = "probs")
test$probs <- round(test$predicted_probs * 100, 1)
test
roc_curve <- roc(response = test$y_numeric, predictor = test$predicted_probs)
plot(roc_curve, main = "ROC Curve for Multinomial Logistic Regression", col = "blue", lwd = 2)
print(paste("AUC:", auc(roc_curve)))
roc_metrics <- coords(roc_curve, x = "all", ret = c("threshold", "sensitivity", "specificity"))
roc_metrics$threshold <- as.numeric(roc_metrics$threshold)  # Ensure threshold is numeric
ggplot(roc_metrics, aes(x = threshold)) +
geom_line(aes(y = sensitivity, color = "Sensitivity")) +
geom_line(aes(y = specificity, color = "Specificity")) +
labs(title = "Sensitivity and Specificity vs. Threshold",
x = "Threshold", y = "Metric Value") +
scale_color_manual(name = "Metrics", values = c("Sensitivity" = "red", "Specificity" = "blue"))
cat("optimal threshold",roc_metrics$threshold[which.min(abs(roc_metrics$sensitivity - roc_metrics$specificity))])
test$predicted_class <- ifelse(test$predicted_probs >= roc_metrics$threshold[which.min(abs(roc_metrics$sensitivity - roc_metrics$specificity))], 1, 0)
library(caret)
conf_matrix <- confusionMatrix(
factor(test$predicted_class, levels = c(0, 1)),
factor(test$y_numeric, levels = c(0, 1)))
confusion_table <- as.data.frame.matrix(conf_matrix$table)
rownames(confusion_table) <- c("Actual: Non-delinquent", "Actual: Delinquent")
colnames(confusion_table) <- c("Predicted: Non-delinquent", "Predicted: Delinquent")
print("Confusion Matrix:")
print(confusion_table)
true_positives <- confusion_table[2, 2]  # Delinquent correctly classified
false_positives <- confusion_table[1, 2] # Non-delinquent misclassified as delinquent
true_negatives <- confusion_table[1, 1]  # Non-delinquent correctly classified
false_negatives <- confusion_table[2, 1] # Delinquent missed
library(car)
# Create the design matrix (without the response variable)
X <- model.matrix(~ feature_1 + feature_2_impute + feature_3_impute + feature_4, data=cleaned_unnormalized_train)
# Calculate the VIF for each predictor
vif_values <- diag(solve(cor(X[, -1])))  # Exclude intercept column
names(vif_values) <- colnames(X)[-1]    # Assign names
print(vif_values)
library(corrplot)
cor_matrix <- cor(cleaned_unnormalized_train[, c("feature_1", "feature_2_impute", "feature_3_impute", "feature_4")])
corrplot(cor_matrix,
method = "color",        # Use colored squares
col = colorRampPalette(c("white", "red"))(200),  # White to red gradient
type = "upper",          # Show only the upper triangle
tl.col = "black",        # Text labels in black
tl.srt = 45,             # Rotate labels
addCoef.col = "black",   # Display correlation coefficients in black
number.cex = 0.8)        # Adjust size of numbers
library(tidyverse)
train <- read.csv(file="FITB_train.csv",header=TRUE)
test <- read.csv(file="FITB_test.csv",header=TRUE)
train_unmodified <- train
library(ggplot2)
ggplot(data=train,aes(feature_3)) + geom_density()
ggplot(data=test,aes(feature_3)) + geom_density()
train$key <- row.names(train)
feature_3_winsor <- data.frame(feature_3 = train$feature_3, key = row.names(train))
feature_3_winsor_clean <- na.omit(feature_3_winsor)
feature_3_winsor_clean <- feature_3_winsor_clean %>%
mutate(z_score = (feature_3 - mean(feature_3)) / sd(feature_3),percentile = ecdf(feature_3)(feature_3) * 100)
feature_3_winsor_df <- feature_3_winsor_clean[!(feature_3_winsor_clean[, 4] < 1 | feature_3_winsor_clean[, 4] > 99), ]
non_matching_keys <- anti_join(train, feature_3_winsor_df, by = "key")$key
train <- train %>% mutate(feature_3 = ifelse(key %in% non_matching_keys, NA, feature_3))
colnames(train)[3] <- "feature_3_winsor"
ggplot(data=train,aes(feature_3_winsor)) + geom_density()
train[is.na(train[,3]),3] <- median(feature_3_winsor_clean$feature_3)
colnames(train)[3] <- "feature_3_impute"
test[is.na(test[,3]),3] <- median(feature_3_winsor_clean$feature_3)
colnames(test)[3] <- "feature_3_impute"
train$date <- format(as.Date(train$date, format = "%Y-%m-%d"), "%Y")
library(dplyr)
train <- train %>%
arrange(id, date) %>% # Sort by id and date
group_by(id) %>%
mutate(feature_2 = ifelse(is.na(feature_2),
lead(feature_2, order_by = date), # Try next year
feature_2)) %>%
mutate(feature_2 = ifelse(is.na(feature_2),
lag(feature_2, order_by = date), # Try previous year
feature_2))
colnames(train)[2] <- "feature_2_impute"
test <- test %>%
arrange(id, date) %>% # Sort by id and date
group_by(id) %>%
mutate(feature_2 = ifelse(is.na(feature_2),
lead(feature_2, order_by = date), # Try next year
feature_2)) %>%
mutate(feature_2 = ifelse(is.na(feature_2),
lag(feature_2, order_by = date), # Try previous year
feature_2))
colnames(test)[2] <- "feature_2_impute"
train <- na.omit(train)
test <- na.omit(test)
cleaned_unnormalized_train <- train
library(ggplot2)
ggplot(data=train,aes(x=feature_1)) + geom_density() + geom_density(data=train,aes(x=feature_2_impute)) + geom_density(data=train,aes(x=feature_3_impute))
train
library(dplyr)
train <- train %>%
mutate(across(c(feature_1, feature_2_impute, feature_3_impute, feature_4),
~ (.x - mean(.x, na.rm = TRUE)) / sd(.x, na.rm = TRUE)))
train
ggplot(data=train,aes(x=feature_1)) + geom_density() + geom_density(data=train,aes(x=feature_2_impute)) + geom_density(data=train,aes(x=feature_3_impute))
colnames(train) <- c("feature_1_standard","feature_2_standard","feature_3_standard","feature_4_standard","id","date","y","key")
test <- test %>%
mutate(across(c(feature_1, feature_2_impute, feature_3_impute, feature_4),
~ (.x - mean(.x, na.rm = TRUE)) / sd(.x, na.rm = TRUE)))
ggplot(data=test,aes(x=feature_1,color="blue")) + geom_density() + geom_density(data=test,aes(x=feature_2_impute,color="red")) + geom_density(data=test,aes(x=feature_3_impute,color="green"))
colnames(test) <- c("feature_1_standard","feature_2_standard","feature_3_standard","feature_4_standard","id","date","y")
write.csv(file="final_train.csv",train)
library(nnet)
train$y <- as.numeric(as.character(factor(train$y, levels = c("90+DPD", "active"), labels = c(1, 0))))
delinquency_model <- multinom(y ~ feature_1_standard + feature_2_standard + feature_3_standard + feature_4_standard, data=train,family=binomial())
summary(delinquency_model)
library(pROC)
library(ggplot2)
test$predicted_y <- predict(delinquency_model, newdata = test, type = "class")
test$y_numeric <- as.numeric(as.character(factor(test$y, levels = c("90+DPD", "active"), labels = c(1, 0))))
test$predicted_probs <- predict(delinquency_model, newdata = test, type = "probs")
test$probs <- round(test$predicted_probs * 100, 1)
test
roc_curve <- roc(response = test$y_numeric, predictor = test$predicted_probs)
plot(roc_curve, main = "ROC Curve for Multinomial Logistic Regression", col = "blue", lwd = 2)
print(paste("AUC:", auc(roc_curve)))
roc_metrics <- coords(roc_curve, x = "all", ret = c("threshold", "sensitivity", "specificity"))
roc_metrics$threshold <- as.numeric(roc_metrics$threshold)  # Ensure threshold is numeric
ggplot(roc_metrics, aes(x = threshold)) +
geom_line(aes(y = sensitivity, color = "Sensitivity")) +
geom_line(aes(y = specificity, color = "Specificity")) +
labs(title = "Sensitivity and Specificity vs. Threshold",
x = "Threshold", y = "Metric Value") +
scale_color_manual(name = "Metrics", values = c("Sensitivity" = "red", "Specificity" = "blue"))
cat("optimal threshold",roc_metrics$threshold[which.min(abs(roc_metrics$sensitivity - roc_metrics$specificity))])
test$predicted_class <- ifelse(test$predicted_probs >= roc_metrics$threshold[which.min(abs(roc_metrics$sensitivity - roc_metrics$specificity))], 1, 0)
library(caret)
conf_matrix <- confusionMatrix(
factor(test$predicted_class, levels = c(0, 1)),
factor(test$y_numeric, levels = c(0, 1)))
confusion_table <- as.data.frame.matrix(conf_matrix$table)
rownames(confusion_table) <- c("Actual: Non-delinquent", "Actual: Delinquent")
colnames(confusion_table) <- c("Predicted: Non-delinquent", "Predicted: Delinquent")
print("Confusion Matrix:")
print(confusion_table)
true_positives <- confusion_table[2, 2]  # Delinquent correctly classified
false_positives <- confusion_table[1, 2] # Non-delinquent misclassified as delinquent
true_negatives <- confusion_table[1, 1]  # Non-delinquent correctly classified
false_negatives <- confusion_table[2, 1] # Delinquent missed
library(car)
# Create the design matrix (without the response variable)
X <- model.matrix(~ feature_1 + feature_2_impute + feature_3_impute + feature_4, data=cleaned_unnormalized_train)
# Calculate the VIF for each predictor
vif_values <- diag(solve(cor(X[, -1])))  # Exclude intercept column
names(vif_values) <- colnames(X)[-1]    # Assign names
print(vif_values)
library(corrplot)
cor_matrix <- cor(cleaned_unnormalized_train[, c("feature_1", "feature_2_impute", "feature_3_impute", "feature_4")])
corrplot(cor_matrix,
method = "color",        # Use colored squares
col = colorRampPalette(c("white", "red"))(200),  # White to red gradient
type = "upper",          # Show only the upper triangle
tl.col = "black",        # Text labels in black
tl.srt = 45,             # Rotate labels
addCoef.col = "black",   # Display correlation coefficients in black
number.cex = 0.8)        # Adjust size of numbers
