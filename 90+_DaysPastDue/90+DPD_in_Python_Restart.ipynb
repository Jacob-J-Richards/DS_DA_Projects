{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "os.chdir(\"/Users/jacobrichards/Desktop/DS_DA_Projects/90+_DaysPastDue/90+DPD_files\")\n",
    "\n",
    "train = pd.read_csv(\"FITB_train.csv\", na_values=[\"\", \"NA\"])\n",
    "\n",
    "test = pd.read_csv(\"FITB_test.csv\", na_values=[\"\", \"NA\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check the distributions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "features = [\"feature_1\", \"feature_2\", \"feature_3\", \"feature_4\"]\n",
    "\n",
    "plt.figure(figsize=(10, 6), dpi=300)  # High-quality figure\n",
    "\n",
    "# Create boxplot\n",
    "train[features].boxplot(grid=False, vert=False)\n",
    "plt.title(\"Boxplots of Features\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outliers of feature 3 will yeild disproportioate weight to the model parameter for feature 3 in whatever model we build. We we need to remove these outliers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove top 1 percent\n",
    "threshold = train['feature_3'].quantile(0.99)\n",
    "\n",
    "train.loc[train['feature_3'] > threshold, 'feature_3'] = pd.NA\n",
    "\n",
    "\n",
    "# remove bottom 1 percent \n",
    "threshold = train['feature_3'].quantile(0.01)\n",
    "\n",
    "train.loc[train['feature_3'] < threshold, 'feature_3'] = pd.NA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "features = [\"feature_1\", \"feature_2\", \"feature_3\", \"feature_4\"]\n",
    "\n",
    "plt.figure(figsize=(10, 6), dpi=300)  # High-quality figure\n",
    "\n",
    "# Create boxplot\n",
    "train[features].boxplot(grid=False, vert=False)\n",
    "plt.title(\"Boxplots of Features\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "replace those values we just removed with median "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_value = train['feature_3'].median()\n",
    "\n",
    "train['feature_3'].fillna(median_value, inplace=True)\n",
    "\n",
    "train.rename(columns={'feature_3': 'feature_3_impute'}, inplace=True)\n",
    "\n",
    "\n",
    "median_value = test['feature_3'].median()\n",
    "\n",
    "test['feature_3'].fillna(median_value, inplace=True)\n",
    "\n",
    "test.rename(columns={'feature_3': 'feature_3_impute'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sort by ID and date so that if feature 2 is missing we can replace it with the next year or the previous years value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['date'] = pd.to_datetime(train['date']).dt.year\n",
    "test['date'] = pd.to_datetime(test['date']).dt.year\n",
    "\n",
    "def impute_feature_2(df):\n",
    "    df = df.sort_values(by=['id', 'date'])  \n",
    "    df['feature_2'] = df['feature_2'].fillna(method='bfill') \n",
    "    df['feature_2'] = df['feature_2'].fillna(method='ffill') \n",
    "    return df\n",
    "\n",
    "train = train.groupby('id', group_keys=False).apply(impute_feature_2)\n",
    "test = test.groupby('id', group_keys=False).apply(impute_feature_2)\n",
    "\n",
    "train.rename(columns={'feature_2': 'feature_2_impute'}, inplace=True)\n",
    "test.rename(columns={'feature_2': 'feature_2_impute'}, inplace=True)\n",
    "\n",
    "train = train.dropna(subset=['feature_2_impute'])\n",
    "test = test.dropna(subset=['feature_2_impute'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encode outocomes to 1 and 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['y'] = train['y'].apply(lambda x: 1 if x == \"90+DPD\" else 0 if x == \"active\" else x)\n",
    "test['y'] = test['y'].apply(lambda x: 1 if x == \"90+DPD\" else 0 if x == \"active\" else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all the NA's are gone, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preliminary diagnostics for suitability of Logistic Regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "predictor = train[['feature_1', 'feature_2_impute', 'feature_3_impute', 'feature_4']]\n",
    "response = train['y']\n",
    "\n",
    "predictor_with_const = sm.add_constant(predictor)\n",
    "\n",
    "logit_model = sm.Logit(response, predictor_with_const)\n",
    "result = logit_model.fit()\n",
    "\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pseduo R-square is 0.2260, which begs the question of are these variables actually predictors of the response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the log(p/1-p) curves for each predictor from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def plot_logit_curve_with_smoother(result, predictor, feature_name):\n",
    "    \"\"\"\n",
    "    Plot the logit curve (odds) for a given feature with scatter points and a smoother.\n",
    "\n",
    "    Parameters:\n",
    "    - result: Fitted logistic regression model.\n",
    "    - predictor: Training dataset of predictors.\n",
    "    - feature_name: The feature for which to plot the logit curve.\n",
    "    \"\"\"\n",
    "    # Generate a range of values for the selected feature\n",
    "    feature_values = np.linspace(predictor[feature_name].min(), predictor[feature_name].max(), 100)\n",
    "    \n",
    "    # Keep other predictors fixed at their mean\n",
    "    fixed_predictors = predictor.mean(axis=0).copy()\n",
    "    \n",
    "    # Calculate the logit (odds) for each feature value\n",
    "    log_odds = []\n",
    "    for value in feature_values:\n",
    "        temp_predictors = fixed_predictors.copy()\n",
    "        temp_predictors[feature_name] = value\n",
    "        \n",
    "        # Add constant and ensure shape matches model coefficients\n",
    "        predictors_with_const = sm.add_constant(temp_predictors.values.reshape(1, -1), has_constant='add')\n",
    "        logit = np.dot(predictors_with_const, result.params)\n",
    "        #odds = np.exp(logit)  this is ploting the log(p/1-p) i just ommited the exp()\n",
    "        log_odds.append(logit[0])\n",
    "    \n",
    "    # Create a DataFrame for plotting\n",
    "    plot_df = pd.DataFrame({\n",
    "        feature_name: feature_values,\n",
    "        'log_odds': log_odds\n",
    "    })\n",
    "    \n",
    "    # Plot the logit curve with points and smoother\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(x=feature_name, y='log_odds', data=plot_df, color='blue', s=50, label='Logit Points')\n",
    "    sns.regplot(x=feature_name, y='log_odds', data=plot_df, scatter=False, lowess=True, \n",
    "                color='red', line_kws={'lw': 2}, label='Smoothed Logit Curve')\n",
    "    \n",
    "    plt.title(f'Logit Curve (Odds) with Smoother for {feature_name}')\n",
    "    plt.xlabel(feature_name)\n",
    "    plt.ylabel('Log(P / 1-P)')\n",
    "    plt.axhline(1, color='grey', linestyle='--', label='Odds = 1')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "plot_logit_curve_with_smoother(result, predictor, 'feature_1')\n",
    "plot_logit_curve_with_smoother(result, predictor, 'feature_2_impute')\n",
    "plot_logit_curve_with_smoother(result, predictor, 'feature_3_impute')\n",
    "plot_logit_curve_with_smoother(result, predictor, 'feature_4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to verify that the predictor response relationship demonstraighted in the models logit curves are actually representative of the relationship of the variables within the data. That is to say, the model shows predictive power, but is that actually the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are these variables actually predictors of the outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import pointbiserialr\n",
    "\n",
    "def plot_ratio_positive_negative_with_corr_binned(df, continuous_var, categorical_var='target', positive_value=1, negative_value=0, bins=10):\n",
    "    \"\"\"\n",
    "    Plot the ratio of positive to total outcomes for a binned continuous variable with a scatter plot\n",
    "    and display the correlation coefficient.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame containing the data.\n",
    "    - continuous_var: Name of the continuous predictor column.\n",
    "    - categorical_var: Name of the binary target column.\n",
    "    - positive_value: Value representing positive outcomes (default=1).\n",
    "    - negative_value: Value representing negative outcomes (default=0).\n",
    "    - bins: Number of bins to divide the continuous variable (default=10).\n",
    "    \"\"\"\n",
    "    # Bin the continuous variable\n",
    "    df['binned'] = pd.cut(df[continuous_var], bins=bins, include_lowest=True)\n",
    "    \n",
    "    # Group by binned variable and category to get counts\n",
    "    grouped = df.groupby(['binned', categorical_var]).size().reset_index(name='count')\n",
    "    \n",
    "    # Pivot so we have positive and negative counts\n",
    "    pivot = grouped.pivot(index='binned', columns=categorical_var, values='count').fillna(0)\n",
    "    \n",
    "    # Extract positive and total counts\n",
    "    positive_counts = pivot[positive_value] if positive_value in pivot.columns else 0\n",
    "    total_counts = positive_counts + pivot[negative_value] if negative_value in pivot.columns else positive_counts\n",
    "\n",
    "    # To avoid division by zero, add a small epsilon\n",
    "    epsilon = 1e-9\n",
    "    ratio = positive_counts / (total_counts + epsilon)\n",
    "\n",
    "    # Create a DataFrame for plotting\n",
    "    plot_df = pd.DataFrame({\n",
    "        'binned': ratio.index.astype(str),  # Convert bin intervals to strings for plotting\n",
    "        'ratio_positive_negative': ratio.values,\n",
    "        continuous_var: [interval.mid for interval in ratio.index]  # Extract midpoints for x-axis\n",
    "    })\n",
    "    \n",
    "    # Calculate Point-Biserial Correlation (on original data)\n",
    "    correlation, p_value = pointbiserialr(df[continuous_var], df[categorical_var])\n",
    "    \n",
    "    # Plot the points and smoother\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Scatter plot of ratio points\n",
    "    sns.scatterplot(x=continuous_var, y='ratio_positive_negative', data=plot_df, color='blue', s=50, label='Data Points')\n",
    "\n",
    "    # Add a smooth trend line (lowess)\n",
    "    sns.regplot(x=continuous_var, y='ratio_positive_negative', data=plot_df, \n",
    "                scatter=False, lowess=True, color='red', line_kws={'lw': 2}, label='Lowess Smoother')\n",
    "\n",
    "    # Add the correlation coefficient as text on the plot\n",
    "    plt.text(0.05, 0.95, f'Point-Biserial Corr: {correlation:.4f}\\nP-Value: {p_value:.4e}', \n",
    "             transform=plt.gca().transAxes, fontsize=12, verticalalignment='top', bbox=dict(facecolor='white', alpha=0.8))\n",
    "    \n",
    "    # Final plot settings\n",
    "    plt.title(f'Binned Ratio of Positive Outcomes and Correlation by {continuous_var}')\n",
    "    plt.xlabel(continuous_var)\n",
    "    plt.ylabel('Ratio (Positive / Total)')\n",
    "    plt.ylim(0, 1)  # Ratio ranges between 0 and 1\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "plot_ratio_positive_negative_with_corr_binned(train, 'feature_1', 'y', positive_value=1, negative_value=0, bins=10)\n",
    "plot_ratio_positive_negative_with_corr_binned(train, 'feature_2_impute', 'y', positive_value=1, negative_value=0, bins=10)\n",
    "plot_ratio_positive_negative_with_corr_binned(train, 'feature_3_impute', 'y', positive_value=1, negative_value=0, bins=10)\n",
    "plot_ratio_positive_negative_with_corr_binned(train, 'feature_4', 'y', positive_value=1, negative_value=0, bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature 2 is alright but that's not enough, checking the distributions next to see if that has what we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "def plot_combined_box_plots_2x2(df, predictor_vars, categorical_var='target'):\n",
    "    \"\"\"\n",
    "    Create a 2x2 grid of box plots for each predictor variable, grouped by the categorical outcome,\n",
    "    and include mean annotations for each group within each plot.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame containing the data.\n",
    "    - predictor_vars: List of predictor variables for which box plots will be created.\n",
    "    - categorical_var: The categorical outcome variable to group by.\n",
    "    \"\"\"\n",
    "    # Melt the dataframe for easier faceted plotting\n",
    "    melted_df = df[predictor_vars + [categorical_var]].melt(\n",
    "        id_vars=categorical_var,\n",
    "        var_name=\"Predictor\",\n",
    "        value_name=\"Value\"\n",
    "    )\n",
    "    \n",
    "    # Create a facet grid with box plots\n",
    "    g = sns.catplot(\n",
    "        data=melted_df,\n",
    "        x=categorical_var,\n",
    "        y=\"Value\",\n",
    "        col=\"Predictor\",\n",
    "        kind=\"box\",\n",
    "        height=4,\n",
    "        aspect=0.9,\n",
    "        col_wrap=2,  # Ensures a 2x2 grid\n",
    "        sharey=False,\n",
    "    )\n",
    "    \n",
    "    # Annotate each plot with means\n",
    "    for ax, predictor in zip(g.axes.flat, predictor_vars):\n",
    "        category_means = df.groupby(categorical_var)[predictor].mean()\n",
    "        mean_text = \"\\n\".join([f\"{cat}: {mean:.2f}\" for cat, mean in category_means.items()])\n",
    "        ax.text(\n",
    "            0.95, 0.95,\n",
    "            mean_text,\n",
    "            transform=ax.transAxes,\n",
    "            fontsize=9,\n",
    "            verticalalignment=\"top\",\n",
    "            horizontalalignment=\"right\",\n",
    "            bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.7)\n",
    "        )\n",
    "    \n",
    "    # Set titles and adjust layout\n",
    "    g.fig.subplots_adjust(top=0.9)\n",
    "    g.fig.suptitle(f'Box Plots of Predictors by {categorical_var}')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "plot_combined_box_plots_2x2(train, predictor_vars, categorical_var='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature 2 isn't bad as a predictor, but this alone will not generate the predictive power we need for this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "due to the many outliers, the plot of the distributions of the variables by outcome does not demonstraight a big difference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def plot_full_pairplot_with_corr(df, continuous_vars, target_var='target', positive_value=1, negative_value=0):\n",
    "    \"\"\"\n",
    "    Generate a Seaborn pair plot with correlation coefficients annotated on the scatter plots,\n",
    "    showing a full matrix.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing the data.\n",
    "    - continuous_vars: List of continuous variable column names.\n",
    "    - target_var: Name of the binary target column (default='target').\n",
    "    - positive_value: Value representing positive outcomes (default=1).\n",
    "    - negative_value: Value representing negative outcomes (default=0).\n",
    "    \"\"\"\n",
    "    # Filter DataFrame for the required columns\n",
    "    pairplot_data = df[continuous_vars + [target_var]].copy()\n",
    "    \n",
    "    # Ensure the target variable is categorical for coloring\n",
    "    pairplot_data[target_var] = pairplot_data[target_var].astype('category')\n",
    "    \n",
    "    # Calculate pairwise correlations\n",
    "    correlation_matrix = pairplot_data[continuous_vars].corr()\n",
    "    \n",
    "    # Create the pair plot\n",
    "    g = sns.pairplot(\n",
    "        pairplot_data,\n",
    "        hue=target_var,\n",
    "        palette={positive_value: 'green', negative_value: 'red'},\n",
    "        diag_kind='kde',\n",
    "        corner=False\n",
    "    )\n",
    "    \n",
    "    # Annotate correlation coefficients\n",
    "    for i, row_var in enumerate(continuous_vars):\n",
    "        for j, col_var in enumerate(continuous_vars):\n",
    "            if i != j:  # Skip diagonal\n",
    "                # Get the current axis\n",
    "                ax = g.axes[i, j]\n",
    "                # Get the correlation value\n",
    "                corr = correlation_matrix.loc[row_var, col_var]\n",
    "                # Annotate the correlation on the scatter plot\n",
    "                ax.annotate(\n",
    "                    f\"Corr: {corr:.2f}\",\n",
    "                    xy=(0.5, 0.1),\n",
    "                    xycoords=\"axes fraction\",\n",
    "                    ha=\"center\",\n",
    "                    fontsize=9,\n",
    "                    color=\"blue\"\n",
    "                )\n",
    "    \n",
    "    # Add a title for context\n",
    "    plt.suptitle(f\"Pair Plot for Predictors Colored by '{target_var}'\", y=1.02, fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "# Continuous variables and target variable\n",
    "# Call the function for the specific data\n",
    "continuous_vars_1 = ['age', 'dist', 'income', 'marital_status', 'gender']\n",
    "target_var = 'target'\n",
    "\n",
    "\n",
    "# You can call the function again for a different set of variables if needed\n",
    "continuous_vars_2 = ['feature_1', 'feature_2_impute', 'feature_3_impute', 'feature_4']\n",
    "response_var_2 = 'y'\n",
    "\n",
    "plot_full_pairplot_with_corr(train, continuous_vars_2, response_var_2, positive_value=1, negative_value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us that feature 2 and feature 4 is where we will be getting predictive power in our model and that this data is suitable for logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_single_scatter_with_corr_by_category(df, feature_x, feature_y, target_var='target', positive_value=1, negative_value=0):\n",
    "    \"\"\"\n",
    "    Plot a full-sized scatter plot of two features with correlation annotated for each category.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing the data.\n",
    "    - feature_x: Name of the x-axis feature.\n",
    "    - feature_y: Name of the y-axis feature.\n",
    "    - target_var: Name of the binary target column.\n",
    "    - positive_value: Value representing positive outcomes.\n",
    "    - negative_value: Value representing negative outcomes.\n",
    "    \"\"\"\n",
    "    # Ensure the target variable is categorical for coloring\n",
    "    df[target_var] = df[target_var].astype('category')\n",
    "\n",
    "    # Calculate correlation for each category\n",
    "    positive_corr = df[df[target_var] == positive_value][feature_x].corr(df[df[target_var] == positive_value][feature_y])\n",
    "    negative_corr = df[df[target_var] == negative_value][feature_x].corr(df[df[target_var] == negative_value][feature_y])\n",
    "\n",
    "    # Create the scatter plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.scatterplot(\n",
    "        data=df,\n",
    "        x=feature_x,\n",
    "        y=feature_y,\n",
    "        hue=target_var,\n",
    "        palette={positive_value: 'green', negative_value: 'red'},\n",
    "        alpha=0.7\n",
    "    )\n",
    "\n",
    "    # Annotate correlation for positive and negative categories\n",
    "    plt.annotate(\n",
    "        f\"Positive Corr ({positive_value}): {positive_corr:.2f}\",\n",
    "        xy=(0.05, 0.9),\n",
    "        xycoords=\"axes fraction\",\n",
    "        ha=\"left\",\n",
    "        fontsize=12,\n",
    "        color=\"green\",\n",
    "        bbox=dict(boxstyle=\"round,pad=0.3\", edgecolor=\"green\", facecolor=\"white\")\n",
    "    )\n",
    "    plt.annotate(\n",
    "        f\"Negative Corr ({negative_value}): {negative_corr:.2f}\",\n",
    "        xy=(0.05, 0.85),\n",
    "        xycoords=\"axes fraction\",\n",
    "        ha=\"left\",\n",
    "        fontsize=12,\n",
    "        color=\"red\",\n",
    "        bbox=dict(boxstyle=\"round,pad=0.3\", edgecolor=\"red\", facecolor=\"white\")\n",
    "    )\n",
    "\n",
    "    # Set plot labels and title\n",
    "    plt.title(f\"Scatter Plot: {feature_x} vs {feature_y} Colored by '{target_var}'\", fontsize=14)\n",
    "    plt.xlabel(feature_x, fontsize=12)\n",
    "    plt.ylabel(feature_y, fontsize=12)\n",
    "    plt.legend(title=target_var, loc='upper right')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "plot_single_scatter_with_corr_by_category(train, 'feature_2_impute', 'feature_4', target_var='y', positive_value=1, negative_value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "that's it we found it. .71 correlation betweeen feature 2 and 4 for their points coresponding to posative outcomes. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
