---
title: "Untitled"
output: html_document
date: "2024-11-05"
---


```{r}
setwd("/Users/jacobrichards/Desktop/DS assesment/DS_Exam_1")
responders_data_sheet <- read.csv(file="data.csv",header=TRUE)
```

Part 1: Hypothesis Testing
Use a Hypothesis Test to determine if responders and non-responders tend to have the same age.
Output: A written summary of the steps of your hypothesis test and the result of your test.



Normality test for ages of entire data set. 
```{r}
responders_data_sheet
population_ages <- responders_data_sheet[,2]
print(min(population_ages))
print(max(population_ages))
hist(population_ages)
shapiro.test(population_ages)
qqnorm(population_ages)
qqline(population_ages, col = "red")
```
By the histogram, shapiro-wilks test for normality, and QQ plot, the ages of the data set are evidently not normally distributed. 


Normality test for responders. 
```{r}
#ages of responders
target_1_ages <- responders_data_sheet[responders_data_sheet[,7] == 1,2]

hist(target_1_ages)
shapiro.test(target_1_ages)
qqnorm(target_1_ages)
qqline(target_1_ages, col = "red")
```
reject null hypotheis that the sample is normally distributed. 


Normality test for non-responders. 
```{r}
#ages of non-responders
target_0_ages <- responders_data_sheet[responders_data_sheet[,7] == 0,2]

hist(target_0_ages)
shapiro.test(target_0_ages)
qqnorm(target_0_ages)
qqline(target_0_ages, col = "red")
```
reject the null hypothesis that the sample is normally distributed. 

Non-parametric hypothesis test, HA: target_1 > target_0
```{r}
wilcox.test(target_1_ages, target_0_ages, alternative = "greater")
```
reject the null hypothesis that the mean age of responders is equal to the mean age of non-responders, and tentatively conclude that the mean age of responders is higher. 

From the histograms for both responders and non-responders, the frequency for both categories decrease from 80 to 95 due to decrease likelihood of being sampled from population by natural causes. 




Part 2: Predictive Modeling
Build a Binary Classifier to help this client better predict responders in future campaigns. 
Output: A brief writeup of your approach, including the algorithm you used, any data analysis or preparation you pursued, and the performance metrics you used to evaluate your final model.


1.) cleaning data 
```{r}
data_income <- responders_data_sheet[responders_data_sheet[, 4] != "", ]

income_levels <- factor(data_income[, "income"],
                                    levels = c("Under $10k", "10-19,999", "20-29,999", "30-39,999",
                                   "40-49,999", "50-59,999", "60-69,999", "70-79,999",
                                   "80-89,999", "90-99,999", "100-149,999", "150 - 174,999",
                                   "175 - 199,999", "200 - 249,999", "250k+"),
                                    ordered = TRUE)

#income levels 1:15, level 1 being under 10k, level 15 being 250k+
data_income$income <- as.numeric(income_levels)

probt_income_men <- data_income[data_income[,5]=="M",]

probt_income_women <- data_income[data_income[,5]=="F",]

responders_data_sheet_men <- data_income[data_income[,5]=="M",]

responders_data_sheet_women <- data_income[data_income[,5]=="F",]
```


we're going to build a logistic regression model to find the probability of response as a function of the other variable. 
```{r}
binary_classifier <- function(predictor, clean_data, color) {
  
  simple_logistic_model <- glm(data = clean_data,
                               target ~ predictor,
                               family = binomial())

  intercept_slope <- coef(simple_logistic_model)
  
  x_evaluation <- seq(min(predictor), max(predictor), length.out = 100)
  
  Prob_by_income <- function(x, intercept_slope) {
    log_odds <- intercept_slope[1] + intercept_slope[2] * x
    odds <- exp(log_odds)
    probability <- odds / (1 + odds)
    return(probability)
  }
  
  probabilities <- sapply(x_evaluation, function(x) Prob_by_income(x, intercept_slope))
  plot(x=x_evaluation,y=probabilities,col=color,xlim=range(min(x_evaluation - 5),max(x_evaluation+5)),ylim=range(0,.5),xlab="")
  legend("topleft", legend = c("Income", "Distance", "Age"),col = c("blue", "black", "red"), lty = 1, lwd = 2)
  min_prob <- min(probabilities)
  min_x <- x_evaluation[which.min(probabilities)]
  
  max_prob <- max(probabilities)
  max_x <- x_evaluation[which.max(probabilities)]
 
  delta_value <- as.numeric(tail(probabilities, 1) - head(probabilities, 1))
 
  delta_text <- paste0("Delta Prob %: ", round(delta_value*100))
  mtext(delta_text, side = 1, line = 3, col = "blue")
  
  points(min_x, min_prob, col = "black", pch = 19)
  
  text(min_x, min_prob - 0.02, labels = paste0("Min: ", round(min_prob, 3)), col = "blue")
  
  points(max_x, max_prob, col = "black", pch = 19)
  
  text(max_x, max_prob - 0.01, labels = paste0("Max: ", round(max_prob, 3)), col = "red")
  
  return(delta_value)

}
```

FOR BOTH GENDERS AND ALL MARITAL STATUS 
```{r}
par(mfrow = c(1, 3))
#income
binary_classifier(data_income$income,data_income,c("blue"))
#distance
binary_classifier(responders_data_sheet$dist,responders_data_sheet,c("black"))
#age
binary_classifier(responders_data_sheet$age,responders_data_sheet,c("red"))
```


```{r}
predictors <- list(probt_income_men$income,probt_income_women$income,
                   responders_data_sheet_men$dist,responders_data_sheet_women$dist,
                   responders_data_sheet_men$age,responders_data_sheet_women$age) 

clean_data_list <- list(probt_income_men,probt_income_women,
                        responders_data_sheet_men,responders_data_sheet_women,
                        responders_data_sheet_men,responders_data_sheet_women)


par(mfrow = c(1,2))

colors <- c("#1E90FF","#87CEFA","#32CD32","#98FB98","#8A2BE2","#DDA0DD")

deltas <- numeric(6)
deltas <- mapply(binary_classifier, predictor = predictors, clean_data = clean_data_list, color = colors)

delta_mat <- matrix(data = deltas, nrow = 3, ncol = 2, byrow = TRUE,
                    dimnames = list(c("income", "distance", "age"), 
                                    c("Male", "Female")))

(delta_mat)

```


Part 3: Data Visualization
Create a visualization of the Cumulative Gains Chart for your model. If you are unfamiliar, see here for more information: https://www.ibm.com/docs/en/spss-statistics/24.0.0?topic=overtraining-cumulative-gains-lift-charts

Output: The image of your Cumulative Gains Chart.






ok so in order to make a cumulative gains chart I need to find the probability of response for every single one of these observations 
not just categories, thus I use the random forest binary classifier. 

binary classifier 
```{r}
setwd("/Users/jacobrichards/Desktop/DS assesment/DS_Exam_1")
responders_data_sheet <- read.csv(file="data.csv",header=TRUE)

responders_data_sheet$gender <- ifelse(responders_data_sheet$gender == "M", "F", "unknown")

responders_data_sheet$marital_status <- ifelse(responders_data_sheet$marital_status == "M", "S", "unknown")

responders_data_sheet$gender <- as.factor(responders_data_sheet$gender)

responders_data_sheet <- responders_data_sheet[responders_data_sheet[, 7] != "", ]

income_levels <- factor(responders_data_sheet[, "income"],
                                    levels = c("Under $10k", "10-19,999", "20-29,999", "30-39,999",
                                   "40-49,999", "50-59,999", "60-69,999", "70-79,999",
                                   "80-89,999", "90-99,999", "100-149,999", "150 - 174,999",
                                   "175 - 199,999", "200 - 249,999", "250k+"),
                                    ordered = TRUE)

responders_data_sheet$income <- as.numeric(income_levels)

responders_data_sheet$income[is.na(responders_data_sheet$income)] <- "unknown"

responders_data_sheet$marital_status <- as.factor(responders_data_sheet$marital_status)

responders_data_sheet$target <- factor(responders_data_sheet$target,levels = c(1,0), labels = c("responce", "no_response"))

responders_data_sheet <- responders_data_sheet[,2:7]


set.seed(123)
trainIndex <- createDataPartition(responders_data_sheet$target, p = 0.8, 
                                  list = FALSE, 
                                  times = 1)

response_train <- responders_data_sheet[trainIndex,]
response_test <- responders_data_sheet[-trainIndex,]


rf_model <- randomForest(target ~ ., data = response_train, importance = TRUE, ntree = 500)


print(rf_model)


# Make predictions on the test set
predictions <- predict(rf_model, newdata = response_test)

# Confusion Matrix
conf_matrix <- confusionMatrix(predictions, response_test$target)
print(conf_matrix)

clean_data <- responders_data_sheet

rf_model <- randomForest(target ~ ., data = clean_data, importance = TRUE, ntree = 500)

print(rf_model)

predicted_probabilities <- predict(rf_model, newdata = clean_data, type = "prob")

response_probabilities <- predicted_probabilities[, "responce"]

obs_with_prediction_prob <- cbind(clean_data,response_probabilities)
```



cumulative sum chart using this model

```{r}

obs_with_prediction_prob



```





Part 4: Brief Writeup
Provide an executive summary of your findings from tasks 1-3. Treat this as if you were sending an email to a teammate or your supervisor bringing them up to speed on your analysis and findings.


