---
title: "Your Document Title"
output:
  md_document:
    variant: gfm
knit: (function(input, ...) { rmarkdown::render(input, ..., output_file = "ReadMe.md") })
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  fig.align = "center",
  fig.width = 8,   
  fig.height = 6, 
  dpi = 300,     
  dev = "png",
  out.width = "70%",
  out.height = "70%"
)

knitr::knit_hooks$set(plot = function(x, options) {
  paste0('<div align="center">\n',
         '<img src="', x, '" width="70%">\n',
         '</div>')
})
```

# Multivariable Logistic Binary Classifier - Delinquency Prediction

The panel data-set contains commercial customers' financial information and days past due indicator from 2000 to 2020. The goal is to build a model to predict when customers will be 90+ days past due **(90+DPD)** on payments.

## Prepare Training Data

```{r}
setwd("/Users/jacobrichards/Desktop/DS_DA_Projects/Delinquency_Prediction")
train <- read.csv(file="/Users/jacobrichards/Desktop/DS_DA_Projects/Delinquency_Prediction/Data_Files/FITB_train.csv",header=TRUE)
test <- read.csv(file="/Users/jacobrichards/Desktop/DS_DA_Projects/Delinquency_Prediction/Data_Files/FITB_test.csv",header=TRUE)
```

From checking the distribution of the data, if you look carefully you can see that the distribution of feature 3 (blue) has a lot of values in the extreme right tail.

```{r}
library(ggplot2)
ggplot() + geom_density(data=train, aes(x=feature_3), color="blue") +
           geom_density(data=train, aes(x=feature_2), color="red") +
           geom_density(data=train, aes(x=feature_1), color="green") +
           geom_density(data=train, aes(x=feature_4), color="purple") +
           theme_minimal()
```

This is just noise that will hurt weaken our model, so we remove the top and bottom 1% from the tails of feature 3. **"Winsorize"** feature 3.

```{r}
library(dplyr)
train$key <- row.names(train)
feature_3_winsor <- data.frame(feature_3 = train$feature_3, key = row.names(train))
feature_3_winsor_clean <- na.omit(feature_3_winsor)

feature_3_winsor_clean <- feature_3_winsor_clean %>%
  mutate(z_score = (feature_3 - mean(feature_3)) / sd(feature_3),percentile = ecdf(feature_3)(feature_3) * 100)

feature_3_winsor_df <- feature_3_winsor_clean[!(feature_3_winsor_clean[, 4] < 1 | feature_3_winsor_clean[, 4] > 99), ]

non_matching_keys <- anti_join(train, feature_3_winsor_df, by = "key")$key

train <- train %>% mutate(feature_3 = ifelse(key %in% non_matching_keys, NA, feature_3))

colnames(train)[3] <- "feature_3_winsor"
```

We need to fill the values we just removed, so we replace them with the median of the values remaining.

```{r}
train[is.na(train[,3]),3] <- median(feature_3_winsor_clean$feature_3)

colnames(train)[3] <- "feature_3_impute"

test[is.na(test[,3]),3] <- median(feature_3_winsor_clean$feature_3)
colnames(test)[3] <- "feature_3_impute"
```

feature 2 has missing values, so we will Impute these missing values with value from the next or the previous year of that same ID.

I.e. if feature 2 for DEC 31 2001 is missing for ID number 5021, then replace with value from DEC 31 2002 or DEC 31 2000 of feature 2 of that same ID number (individual).

```{r}
train$date <- format(as.Date(train$date, format = "%Y-%m-%d"), "%Y")

train <- train %>%
  arrange(id, date) %>% # Sort by id and date
  group_by(id) %>%
  mutate(feature_2 = ifelse(is.na(feature_2),
                            lead(feature_2, order_by = date), # Try next year
                            feature_2)) %>%
  mutate(feature_2 = ifelse(is.na(feature_2),
                            lag(feature_2, order_by = date), # Try previous year
                            feature_2))

colnames(train)[2] <- "feature_2_impute"


test <- test %>%
  arrange(id, date) %>% 
  group_by(id) %>%
  mutate(feature_2 = ifelse(is.na(feature_2),
                            lead(feature_2, order_by = date), # Try next year
                            feature_2)) %>%
  mutate(feature_2 = ifelse(is.na(feature_2),
                            lag(feature_2, order_by = date), # Try previous year
                            feature_2))

colnames(test)[2] <- "feature_2_impute"

train <- na.omit(train)
test <- na.omit(test)
print(head(train,5))
```

Our variables "features" all represent different financial measurements numerically. In producing a probabilistic model model for outcome of delinquency the actual value of these variables is meaningless as we don't even know what they represent, what can be meaningful from them however is how each value relates to the other values of it's own variable.

Therefore, we should **Normalize** or **Standardize** the variables, by re-assigning each value with it's corresponding z-score within it's distribution.

```{r}
library(dplyr)
train <- train %>%
  mutate(across(c(feature_1, feature_2_impute, feature_3_impute, feature_4), 
                ~ (.x - mean(.x, na.rm = TRUE)) / sd(.x, na.rm = TRUE)))

colnames(train) <- c("feature_1_standard","feature_2_standard","feature_3_standard","feature_4_standard","id","date","y","key")

test <- test %>%
  mutate(across(c(feature_1, feature_2_impute, feature_3_impute, feature_4), 
                ~ (.x - mean(.x, na.rm = TRUE)) / sd(.x, na.rm = TRUE)))

colnames(test) <- c("feature_1_standard","feature_2_standard","feature_3_standard","feature_4_standard","id","date","y")

ggplot() + 
  geom_density(data = train, aes(x = feature_3_standard), color = "blue") +
  geom_density(data = train, aes(x = feature_2_standard), color = "red") +
  geom_density(data = train, aes(x = feature_1_standard), color = "green") +
  geom_density(data = train, aes(x = feature_4_standard), color = "purple") +
  theme_minimal() +
  labs(x = "Normalized Features")

print(head(train,5))
```

The preparation of the training data is complete. The essential information of the data has been revealed and can now be utilized to produce a model for delinquency of payment prediction.

## Building The Model

Building a logistic regression model where features 1 to 4 are independent variables and column y is our dependent variable being known outcome of payment delinquency.

Given historical data of these variables and their effect on the outcome of delinquency, we can produce a model which will generate a probability that a customer will be 90+ days past due on payment to be used on future customers to predict delinquency status.

Our model is built from 4000 observations which has been prepared to reveal the cause and effect nature of the variables and delinquency outcome.

For explanation of binary classifiers see the following invaluable resource: <https://seantrott.github.io/binary_classification_R/>

```{r}
library(nnet)
train$y <- as.numeric(as.character(factor(train$y, levels = c("90+DPD", "active"), labels = c(1, 0))))
delinquency_model <- multinom(y ~ feature_1_standard + feature_2_standard + feature_3_standard + feature_4_standard, data=train,family=binomial())
```

When the model produces a probability of an individual being delinquent on payments, we will have to decide at what probability we conclude that that individual will indeed be delinquent. Determination of this probability value (decision threshold) is extremely important to the effectiveness of the model.

## Fitting The Model

**decision threshold:** The probability value for which a customer is concluded will be **90+ DPD** when the model produces a probability that the customer will be **90+ DPD** greater than or equal to it.

Since the outcome of delinquency is known in our testing data, we can directly compare the model's predicted delinquency outcome against the actual delinquency outcome to evaluate the accuracy of our model. As well, how our set decision threshold effects the accuracy of the model.

To determine this **decision threshold**, we evaluate the model on our testing data, our testing data originated from the same data set as our training data. However, since the testing data was not included in producing the model, the accuracy of this evaluation will have direct implications to the models accuracy applied to future data.

```{r}
(head(test,5))
```

To evaluate the effectiveness of the model and optimal **decision threshold**, we produce an ROC curve from the predicted outcome produced by the model and known outcome.

```{r}
    library(pROC)
    test$predicted_y <- predict(delinquency_model, newdata = test, type = "class")
    test$y_numeric <- as.numeric(as.character(factor(test$y, levels = c("90+DPD", "active"), labels = c(1, 0))))
    test$Probability <- predict(delinquency_model, newdata = test, type = "probs")
    options(digits = 4)
    
    roc_curve <- roc(response = test$y_numeric, predictor = test$Probability)

    
    roc_metrics <- coords(roc_curve, x = "all", ret = c("threshold", "sensitivity", "specificity"))
    
    auc_value <- auc(roc_curve) 
    
    roc_data <- data.frame( TPR = roc_metrics$sensitivity, FPR = 1 - roc_metrics$specificity, threshold = roc_metrics$threshold)

ggplot(roc_data, aes(x = FPR, y = TPR, color = threshold)) +
  geom_line(size = 1) +
  geom_abline(linetype = "dashed", color = "gray") +
  # Add the perfect model line
  geom_line(data = data.frame(FPR = c(0, 0, 1), TPR = c(0, 1, 1)), aes(x = FPR, y = TPR), 
            color = "blue", size = 1, linetype = "dotted") +
  labs(title = "ROC Curve for Multinomial Logistic Regression",
       x = "False Positive Rate (1 - Specificity)", y = "True Positive Rate (Sensitivity)", 
       caption = paste("AUC:", round(auc_value, 4)), color = "Decision Threshold") +
  scale_color_gradientn(colors = rev(rainbow(100))) + 
  coord_fixed() + xlim(0, 1) + ylim(0, 1) + theme_minimal() +
  theme(plot.caption = element_text(hjust = 0.5, size = 12))
```

**Sensitivity:** The proportion of positive outcomes (delinquent) correctly identified by the model

*what proportion of delinquencies were caught by the model.*

**Specificity:** The proportion of negative outcomes (not delinquent) correctly identified by the model

*what proportion of people who did not go delinquent on payments were not mis-categorized as delinquent.*

**False Positive Rate:** The proportion of negative outcomes (not delinquent) incorrectly identified by the model as delinquent. Notice how this is the compliment to **Specificity**.

**Decision Threshold:** The probability value of which observations assained probability values of delinquency from the model equal to or greater than the value are classified as delinquent (1), if less than this value they are classified as non-delinquent (0).

**AUC:** The area under the curve is used as a metric for effectiveness of the model, the blue dotted would be a perfect model which captures 100% of the area under it's curve (everything right), and the grey dotted is random chance (coin toss) which would only capture 50% of the area (half right,half wrong).

The **AUC** of our model is 0.8211.

The ROC curve is the plot of the **True Positive Rate** TRP **(Sensitivity)** and **False Positive Rate** FPR (1- **Specificity**) as a function of decision threshold displayed by color gradient along the curve.

For example: when the decision threshold is around 0.4 (blue above cyan), the **False Positive Rate** around 0.125 and the **True Positive Rate** is 0.4, meaning that when the decision threshold value is relatively high, the false positive rate is relatively low however only 40% of delinquencies are being caught by the model.

In contrast, if you look at decision threshold around 0.20 (hot pink), the **False Positive Rate** around 0.40 and the **True Positive Rate** is 0.90, such that we catch 90% of the delinquencies but 40% of non delinquencies were mis-identified as delinquent.

Therefore, the **decision threshold** we choose is a trade off between identifying delinquencies and not mis-identifying non-delinquencies, this **decision threshold** is visually evident in the following plot.

```{r}
ggplot(roc_metrics, aes(x = threshold)) +
    geom_smooth(aes(y = sensitivity, color = "Sensitivity")) +
    geom_smooth(aes(y = specificity, color = "Specificity")) +
    labs(title = "Sensitivity and Specificity vs. Threshold",
    x = "Threshold", y = "Metric Value") +
    scale_color_manual(name = "Metrics", values = c("Sensitivity" = "red", "Specificity" = "blue")) +
    theme_minimal()
```

The balanced **decision threshold** is visually apparent by the intersection of **Sensitivity** and **Specificity** and by the following simple calculation.

```{r}
optimal_threshold <- roc_metrics$threshold[which.min(abs(roc_metrics$sensitivity - roc_metrics$specificity))]
(roc_metrics[roc_metrics$threshold == optimal_threshold,])
```

Confusion matrix displaying the results of balanced decision threshold evaluated on the testing data.

```{r}
library(webshot2)
test$predicted_class <- ifelse(test$Probability >= roc_metrics$threshold[which.min(abs(roc_metrics$sensitivity - roc_metrics$specificity))], 1, 0)

library(caret)
conf_matrix <- confusionMatrix(
  factor(test$predicted_class, levels = c(0, 1)),
  factor(test$y_numeric, levels = c(0, 1)))

confusion_table <- as.data.frame.matrix(conf_matrix$table)
rownames(confusion_table) <- c("Actual: Non-delinquent", "Actual: Delinquent")
colnames(confusion_table) <- c("Predicted: Non-delinquent", "Predicted: Delinquent")
library(gt)

confusion_table_gt <- confusion_table %>%
  tibble::rownames_to_column("Actual") %>%
  gt() %>%
  tab_header(
    title = "Confusion Matrix",
    subtitle = "Performance of Classification Model"
  ) %>%
  fmt_number(
    columns = -1, # Format numeric columns, skipping the 'Actual' column
    decimals = 0
  ) %>%
  cols_label(
    Actual = "",
    `Predicted: Non-delinquent` = "Non-delinquent",
    `Predicted: Delinquent` = "Delinquent"
  )

gtsave(
  data = confusion_table_gt,
  filename = "~/Desktop/DS_DA_Projects/Delinquency_Prediction/ReadMe_files/figure-gfm/confusion_table.png",
  vwidth = 800,  # width in pixels
  vheight = 800  # height in pixels, equal to width for square aspect
)
knitr::include_graphics("~/Desktop/DS_DA_Projects/Delinquency_Prediction/ReadMe_files/figure-gfm/confusion_table.png")
```

## Fitting The Model - Checking for Multicollinearity

```{r}
library(corrplot)
cor_matrix <- cor(train[, c("feature_1_standard", "feature_2_standard", "feature_3_standard", "feature_4_standard")])
corrplot(cor_matrix, 
         method = "color",        
         col = colorRampPalette(c("white", "red"))(200),  
         type = "upper",          
         tl.col = "black",        
         tl.srt = 45,             
         addCoef.col = "black",  
         number.cex = 0.8)

library(corrplot)
cor_matrix <- cor(test[, c("feature_1_standard", "feature_2_standard", "feature_3_standard", "feature_4_standard")])
corrplot(cor_matrix, 
         method = "color",        
         col = colorRampPalette(c("white", "red"))(200),  
         type = "upper",          
         tl.col = "black",        
         tl.srt = 45,             
         addCoef.col = "black",  
         number.cex = 0.8)
```

Features 1 and 3 have significant correlation of 0.62 and 0.71 in the training and testing data respectively.

## Conclusion

Provided that future data sets evaluated by this model have similar multicollinearity as the training and testing data as is expected, this model will maintain relevance in providing predictions that are vastly advantaged to random chance.
