---
title: "Your Document Title"
output:
  md_document:
    variant: gfm
knit: (function(input, ...) { rmarkdown::render(input, ..., output_file = "ReadMe.md") })
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  fig.align = "center",
  fig.width = 8,   
  fig.height = 6, 
  dpi = 300,     
  dev = "png",
  out.width = "70%",
  out.height = "70%"
)

knitr::knit_hooks$set(plot = function(x, options) {
  paste0('<div align="center">\n',
         '<img src="', x, '" width="70%">\n',
         '</div>')
})
```

# Multivariable Logistic Binary Classifier - Delinquency Prediction

The panel data-set contains commercial customers' financial information and days past due indicator from 2000 to 2020. The goal is to build a model to predict when customers will be 90+ days past due **(90+DPD)** on payments.

## Prepare Training Data

```{r}
setwd("/Users/jacobrichards/Desktop/DS_DA_Projects/Delinquency_Prediction")
train <- read.csv(file="/Users/jacobrichards/Desktop/DS_DA_Projects/Delinquency_Prediction/Data_Files/FITB_train.csv",header=TRUE)
test <- read.csv(file="/Users/jacobrichards/Desktop/DS_DA_Projects/Delinquency_Prediction/Data_Files/FITB_test.csv",header=TRUE)
```

From checking the distribution of the data, if you look carefully you can see that the distribution of feature 3 (displayed by the blue curve) has a lot of values in the extreme right tail.

```{r}
library(ggplot2)
ggplot() + geom_density(data=train, aes(x=feature_3), color="blue") +
           geom_density(data=train, aes(x=feature_2), color="red") +
           geom_density(data=train, aes(x=feature_1), color="green") +
           geom_density(data=train, aes(x=feature_4), color="purple") +
           theme_minimal()
```

This many outliers will weaken our model, so we remove the top and bottom percentiles from feature 3. This is known as **Winsorization**.

```{r}
library(dplyr)
train$key <- row.names(train)
feature_3_winsor <- data.frame(feature_3 = train$feature_3, key = row.names(train))
feature_3_winsor_clean <- na.omit(feature_3_winsor)

feature_3_winsor_clean <- feature_3_winsor_clean %>%
  mutate(z_score = (feature_3 - mean(feature_3)) / sd(feature_3),percentile = ecdf(feature_3)(feature_3) * 100)

feature_3_winsor_df <- feature_3_winsor_clean[!(feature_3_winsor_clean[, 4] < 1 | feature_3_winsor_clean[, 4] > 99), ]

non_matching_keys <- anti_join(train, feature_3_winsor_df, by = "key")$key

train <- train %>% mutate(feature_3 = ifelse(key %in% non_matching_keys, NA, feature_3))

colnames(train)[3] <- "feature_3_winsor"
```

We need to fill in the values we just removed, so we replace them with the median of the non-outliers.

```{r}
train[is.na(train[,3]),3] <- median(feature_3_winsor_clean$feature_3)

colnames(train)[3] <- "feature_3_impute"

test[is.na(test[,3]),3] <- median(feature_3_winsor_clean$feature_3)
colnames(test)[3] <- "feature_3_impute"
```

Feature 2 has missing values, so we will Impute them with the feature 2 value from the next year or the previous (if next year is also missing) corresponding to the same ID.

```{r}
train$date <- format(as.Date(train$date, format = "%Y-%m-%d"), "%Y")

train <- train %>%
  arrange(id, date) %>% # Sort by id and date
  group_by(id) %>%
  mutate(feature_2 = ifelse(is.na(feature_2),
                            lead(feature_2, order_by = date), # Try next year
                            feature_2)) %>%
  mutate(feature_2 = ifelse(is.na(feature_2),
                            lag(feature_2, order_by = date), # Try previous year
                            feature_2))

colnames(train)[2] <- "feature_2_impute"


test <- test %>%
  arrange(id, date) %>% 
  group_by(id) %>%
  mutate(feature_2 = ifelse(is.na(feature_2),
                            lead(feature_2, order_by = date), # Try next year
                            feature_2)) %>%
  mutate(feature_2 = ifelse(is.na(feature_2),
                            lag(feature_2, order_by = date), # Try previous year
                            feature_2))

colnames(test)[2] <- "feature_2_impute"

train <- na.omit(train)
test <- na.omit(test)
print(head(train,5))
```

Our features (variables) all represent different financial measurements quantified by vastly different units. In order for these variables to have uniformity, we can reassign each value of each variable with it's corresponding z-score within it's respective distribution.

```{r}
library(dplyr)
train <- train %>%
  mutate(across(c(feature_1, feature_2_impute, feature_3_impute, feature_4), 
                ~ (.x - mean(.x, na.rm = TRUE)) / sd(.x, na.rm = TRUE)))

colnames(train) <- c("feature_1_standard","feature_2_standard","feature_3_standard","feature_4_standard","id","date","y","key")

test <- test %>%
  mutate(across(c(feature_1, feature_2_impute, feature_3_impute, feature_4), 
                ~ (.x - mean(.x, na.rm = TRUE)) / sd(.x, na.rm = TRUE)))

colnames(test) <- c("feature_1_standard","feature_2_standard","feature_3_standard","feature_4_standard","id","date","y")

ggplot() + 
  geom_density(data = train, aes(x = feature_3_standard), color = "blue") +
  geom_density(data = train, aes(x = feature_2_standard), color = "red") +
  geom_density(data = train, aes(x = feature_1_standard), color = "green") +
  geom_density(data = train, aes(x = feature_4_standard), color = "purple") +
  theme_minimal() +
  labs(x = "Normalized Features")

print(head(train,5))
```

The preparation of the training data is complete.

## Building The Model

Building a logistic regression model from features 1 to 4 as continuous independent variables and column y as the binary dependent variable (true/false).

Given historical data of these variable values and the payment status they correspond to, we can produce a model which will generates a probability that a customer will be **90+ days past due** on payments to be used on current or future customers.

For explanation of logistic regression binary classifiers see the following invaluable resource: <https://seantrott.github.io/binary_classification_R/>

```{r}
library(nnet)
train$y <- as.numeric(as.character(factor(train$y, levels = c("90+DPD", "active"), labels = c(1, 0))))
delinquency_model <- multinom(y ~ feature_1_standard + feature_2_standard + feature_3_standard + feature_4_standard, data=train,family=binomial())
```

When the model produces a probability of an individual being **90+DPD**, we will have to decide at what probability we conclude that that individual will be **90+DPD**. The value chosen has great impact on the accuracy of the model.

## Fitting The Model

**decision threshold:** The minimum probability value that a customer will be **90+ DPD** produced by the model at which it is concluded that the customer will be **90+ DPD**.

Since the outcome of whether or not there will be late payments is known in our testing data, we can directly compare the model's predicted outcome with the actual outcome to evaluate the accuracy of our model. As well, we can evaluate the impact of our decision threshold on the models accuracy.

As we evaluate the model on our testing data which was not included in the data used to produce the model, the accuracy of these results will have implications on the accuracy of the model being evaluated on future data. Likewise the decision threshold accuracy found from this evaluation will have similar implications.

```{r}
(head(test,5))
```

To evaluate the effectiveness of the model find the optimal **decision threshold**, we produce an ROC curve.

The ROC curve is a plot of accuracy ratios of the models predictions as a result of decision threshold chosen.

```{r}
    library(pROC)
    test$predicted_y <- predict(delinquency_model, newdata = test, type = "class")
    test$y_numeric <- as.numeric(as.character(factor(test$y, levels = c("90+DPD", "active"), labels = c(1, 0))))
    test$Probability <- predict(delinquency_model, newdata = test, type = "probs")
    options(digits = 4)
    
    roc_curve <- roc(response = test$y_numeric, predictor = test$Probability)

    
    roc_metrics <- coords(roc_curve, x = "all", ret = c("threshold", "sensitivity", "specificity"))
    
    auc_value <- auc(roc_curve) 
    
    roc_data <- data.frame( TPR = roc_metrics$sensitivity, FPR = 1 - roc_metrics$specificity, threshold = roc_metrics$threshold)

ggplot(roc_data, aes(x = FPR, y = TPR, color = threshold)) +
  geom_line(size = 1) +
  geom_abline(linetype = "dashed", color = "gray") +
  # Add the perfect model line
  geom_line(data = data.frame(FPR = c(0, 0, 1), TPR = c(0, 1, 1)), aes(x = FPR, y = TPR), 
            color = "blue", size = 1, linetype = "dotted") +
  labs(title = "ROC Curve for Multinomial Logistic Regression",
       x = "False Positive Rate (1 - Specificity)", y = "True Positive Rate (Sensitivity)", 
       caption = paste("AUC:", round(auc_value, 4)), color = "Decision Threshold") +
  scale_color_gradientn(colors = rev(rainbow(100))) + 
  coord_fixed() + xlim(0, 1) + ylim(0, 1) + theme_minimal() +
  theme(plot.caption = element_text(hjust = 0.5, size = 12))
```

**Sensitivity:** *what proportion of individuals who were **90+ DPD** did the model correctly predict as being **90+ DPD.***

**Specificity:** *what proportion of individuals who were **not 90+ DDP** did the model correctly predict as being **not 90+ DPD.***

**False Positive Rate:** The proportion of negative outcomes (not delinquent) incorrectly identified by the model as delinquent. Notice how this is the compliment to **Specificity**.

**False Positive Rate:** *what proportion of individuals who were **not 90+ DDP** did the model incorrectly predict as being **90+ DPD.***

# STOPPED HERE 

**AUC:** The area under the curve is used as a metric for effectiveness of the model, the blue dotted would be a perfect model which captures 100% of the area under it's curve (everything right), and the grey dotted is random chance (coin toss) which would only capture 50% of the area (half right,half wrong).

The **AUC** of our model is 0.8211.

The ROC curve is the plot of the **True Positive Rate** TRP **(Sensitivity)** and **False Positive Rate** FPR (1- **Specificity**) as a function of decision threshold displayed by color gradient along the curve.

For example: when the decision threshold is around 0.4 (blue above cyan), the **False Positive Rate** around 0.125 and the **True Positive Rate** is 0.4, meaning that when the decision threshold value is relatively high, the false positive rate is relatively low however only 40% of delinquencies are being caught by the model.

In contrast, if you look at decision threshold around 0.20 (hot pink), the **False Positive Rate** around 0.40 and the **True Positive Rate** is 0.90, such that we catch 90% of the delinquencies but 40% of non delinquencies were mis-identified as delinquent.

Therefore, the **decision threshold** we choose is a trade off between identifying delinquencies and not mis-identifying non-delinquencies, this **decision threshold** is visually evident in the following plot.

```{r}
ggplot(roc_metrics, aes(x = threshold)) +
    geom_smooth(aes(y = sensitivity, color = "Sensitivity")) +
    geom_smooth(aes(y = specificity, color = "Specificity")) +
    labs(title = "Sensitivity and Specificity vs. Threshold",
    x = "Threshold", y = "Metric Value") +
    scale_color_manual(name = "Metrics", values = c("Sensitivity" = "red", "Specificity" = "blue")) +
    theme_minimal()
```

The balanced **decision threshold** is visually apparent by the intersection of **Sensitivity** and **Specificity** and by the following simple calculation.

```{r}
optimal_threshold <- roc_metrics$threshold[which.min(abs(roc_metrics$sensitivity - roc_metrics$specificity))]
(roc_metrics[roc_metrics$threshold == optimal_threshold,])
```

Confusion matrix displaying the results of balanced decision threshold evaluated on the testing data.

```{r}
library(webshot2)
test$predicted_class <- ifelse(test$Probability >= roc_metrics$threshold[which.min(abs(roc_metrics$sensitivity - roc_metrics$specificity))], 1, 0)

library(caret)
conf_matrix <- confusionMatrix(
  factor(test$predicted_class, levels = c(0, 1)),
  factor(test$y_numeric, levels = c(0, 1)))

confusion_table <- as.data.frame.matrix(conf_matrix$table)
rownames(confusion_table) <- c("Actual: Non-delinquent", "Actual: Delinquent")
colnames(confusion_table) <- c("Predicted: Non-delinquent", "Predicted: Delinquent")
library(gt)

confusion_table_gt <- confusion_table %>%
  tibble::rownames_to_column("Actual") %>%
  gt() %>%
  tab_header(
    title = "Confusion Matrix",
    subtitle = "Performance of Classification Model"
  ) %>%
  fmt_number(
    columns = -1, # Format numeric columns, skipping the 'Actual' column
    decimals = 0
  ) %>%
  cols_label(
    Actual = "",
    `Predicted: Non-delinquent` = "Non-delinquent",
    `Predicted: Delinquent` = "Delinquent"
  )

gtsave(
  data = confusion_table_gt,
  filename = "~/Desktop/DS_DA_Projects/Delinquency_Prediction/ReadMe_files/figure-gfm/confusion_table.png",
  vwidth = 800,  # width in pixels
  vheight = 800  # height in pixels, equal to width for square aspect
)
knitr::include_graphics("~/Desktop/DS_DA_Projects/Delinquency_Prediction/ReadMe_files/figure-gfm/confusion_table.png")
```

## Fitting The Model - Checking for Multicollinearity

```{r}
library(corrplot)
cor_matrix <- cor(train[, c("feature_1_standard", "feature_2_standard", "feature_3_standard", "feature_4_standard")])
corrplot(cor_matrix, 
         method = "color",        
         col = colorRampPalette(c("white", "red"))(200),  
         type = "upper",          
         tl.col = "black",        
         tl.srt = 45,             
         addCoef.col = "black",  
         number.cex = 0.8)

library(corrplot)
cor_matrix <- cor(test[, c("feature_1_standard", "feature_2_standard", "feature_3_standard", "feature_4_standard")])
corrplot(cor_matrix, 
         method = "color",        
         col = colorRampPalette(c("white", "red"))(200),  
         type = "upper",          
         tl.col = "black",        
         tl.srt = 45,             
         addCoef.col = "black",  
         number.cex = 0.8)
```

Features 1 and 3 have significant correlation of 0.62 and 0.71 in the training and testing data respectively.

## Conclusion

Provided that future data sets evaluated by this model have similar multicollinearity as the training and testing data as is expected, this model will maintain relevance in providing predictions that are vastly advantaged to random chance.
